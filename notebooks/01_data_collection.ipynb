{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28da06f7-d4f7-4411-96ac-3cb276cc17da",
   "metadata": {},
   "source": [
    "# Exploring data about Belrin Restaurants and the respected Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3560bb65-896a-4096-b909-8d732d1df757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMPORT STATUS:\t DONE.\n"
     ]
    }
   ],
   "source": [
    "# Import libraries and packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys, os\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "sys.path.append(project_root)\n",
    "\n",
    "# Machine Learning and Plotting\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Library to handle JSON files\n",
    "import json\n",
    "import requests \n",
    "\n",
    "# Import libraries and packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Machine Learning and Plotting\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Library to handle JSON files\n",
    "import json\n",
    "import requests \n",
    "\n",
    "# NOT IN THE DEPENDENCIES RN!! \n",
    "\n",
    "# Maps visualization:\n",
    "# import folium\n",
    "# Address to geographical data:\n",
    "#from geopy.geocoders import Nominatim\n",
    "\n",
    "# Web Scraping and Reading HTML files\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Used with lists\n",
    "import itertools\n",
    "\n",
    "# To add delay between quries\n",
    "import time\n",
    "\n",
    "# To Ignore Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(\"IMPORT STATUS:\\t DONE.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b8f7aa-4124-4486-9f4e-bdacff49a041",
   "metadata": {},
   "source": [
    "## 1. Exploring berlin_reviews_2 csv dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2282eaf1-3665-43fd-befd-0e48caba6105",
   "metadata": {},
   "outputs": [],
   "source": [
    "berlin_reviews_2 = pd.read_csv('../data/raw/berlin_reviews_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4a6aba1-28ec-4edc-b40d-4ae6faf4ec0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>cuisines</th>\n",
       "      <th>reviews</th>\n",
       "      <th>star rating</th>\n",
       "      <th>page number</th>\n",
       "      <th>data offset</th>\n",
       "      <th>restaurant</th>\n",
       "      <th>dates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1. Focaccino</td>\n",
       "      <td>https://www.tripadvisor.in/Restaurant_Review-g...</td>\n",
       "      <td>Italian, Sicilian</td>\n",
       "      <td>[\"The atmosphere is very classy yet cozy. The ...</td>\n",
       "      <td>4.5 of 5 bubbles</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>['13 November 2023', '23 September 2023', '28 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2. Cafe Couscous - Vege</td>\n",
       "      <td>https://www.tripadvisor.in/Restaurant_Review-g...</td>\n",
       "      <td>Healthy, Mediterranean</td>\n",
       "      <td>[\"The best wraps in Berlin period. I suggest g...</td>\n",
       "      <td>5.0 of 5 bubbles</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>['25 October 2023', '10 August 2023', '9 June ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3. Hackethals</td>\n",
       "      <td>https://www.tripadvisor.in/Restaurant_Review-g...</td>\n",
       "      <td>German, Bar</td>\n",
       "      <td>['I found this place on TripAdvisor and wanted...</td>\n",
       "      <td>4.5 of 5 bubbles</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>['8 November 2023', '4 November 2023', '4 Nove...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4. Restaurant Buschbeck's</td>\n",
       "      <td>https://www.tripadvisor.in/Restaurant_Review-g...</td>\n",
       "      <td>German, European</td>\n",
       "      <td>[\"As my wife is a coeliac I had to research pl...</td>\n",
       "      <td>5.0 of 5 bubbles</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>['24 October 2023', '1 October 2023', '15 Augu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5. 100 Gramm Bar</td>\n",
       "      <td>https://www.tripadvisor.in/Restaurant_Review-g...</td>\n",
       "      <td>Bar, Eastern European</td>\n",
       "      <td>['This place is amaizing! I loved the vibe, th...</td>\n",
       "      <td>5.0 of 5 bubbles</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>['23 August 2023', '23 August 2023', '21 April...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5785</th>\n",
       "      <td>6356. Bao Club</td>\n",
       "      <td>https://www.tripadvisor.in/Restaurant_Review-g...</td>\n",
       "      <td>Chinese, Asian</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>211</td>\n",
       "      <td>6330</td>\n",
       "      <td>6356</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5786</th>\n",
       "      <td>6357. Wurstkessel im KaDeWe</td>\n",
       "      <td>https://www.tripadvisor.in/Restaurant_Review-g...</td>\n",
       "      <td>French, International</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>211</td>\n",
       "      <td>6330</td>\n",
       "      <td>6357</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5787</th>\n",
       "      <td>6358. World of Champagne (Champagne Bar)</td>\n",
       "      <td>https://www.tripadvisor.in/Restaurant_Review-g...</td>\n",
       "      <td>French, Bar</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>211</td>\n",
       "      <td>6330</td>\n",
       "      <td>6358</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5788</th>\n",
       "      <td>6359. Chibo</td>\n",
       "      <td>https://www.tripadvisor.in/Restaurant_Review-g...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>211</td>\n",
       "      <td>6330</td>\n",
       "      <td>6359</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5789</th>\n",
       "      <td>6360. Steckenpferd</td>\n",
       "      <td>https://www.tripadvisor.in/Restaurant_Review-g...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>211</td>\n",
       "      <td>6330</td>\n",
       "      <td>6360</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5790 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         title  \\\n",
       "0                                 1. Focaccino   \n",
       "1                      2. Cafe Couscous - Vege   \n",
       "2                                3. Hackethals   \n",
       "3                    4. Restaurant Buschbeck's   \n",
       "4                             5. 100 Gramm Bar   \n",
       "...                                        ...   \n",
       "5785                            6356. Bao Club   \n",
       "5786               6357. Wurstkessel im KaDeWe   \n",
       "5787  6358. World of Champagne (Champagne Bar)   \n",
       "5788                               6359. Chibo   \n",
       "5789                        6360. Steckenpferd   \n",
       "\n",
       "                                                    url  \\\n",
       "0     https://www.tripadvisor.in/Restaurant_Review-g...   \n",
       "1     https://www.tripadvisor.in/Restaurant_Review-g...   \n",
       "2     https://www.tripadvisor.in/Restaurant_Review-g...   \n",
       "3     https://www.tripadvisor.in/Restaurant_Review-g...   \n",
       "4     https://www.tripadvisor.in/Restaurant_Review-g...   \n",
       "...                                                 ...   \n",
       "5785  https://www.tripadvisor.in/Restaurant_Review-g...   \n",
       "5786  https://www.tripadvisor.in/Restaurant_Review-g...   \n",
       "5787  https://www.tripadvisor.in/Restaurant_Review-g...   \n",
       "5788  https://www.tripadvisor.in/Restaurant_Review-g...   \n",
       "5789  https://www.tripadvisor.in/Restaurant_Review-g...   \n",
       "\n",
       "                    cuisines  \\\n",
       "0          Italian, Sicilian   \n",
       "1     Healthy, Mediterranean   \n",
       "2                German, Bar   \n",
       "3           German, European   \n",
       "4      Bar, Eastern European   \n",
       "...                      ...   \n",
       "5785          Chinese, Asian   \n",
       "5786   French, International   \n",
       "5787             French, Bar   \n",
       "5788                     NaN   \n",
       "5789                     NaN   \n",
       "\n",
       "                                                reviews       star rating  \\\n",
       "0     [\"The atmosphere is very classy yet cozy. The ...  4.5 of 5 bubbles   \n",
       "1     [\"The best wraps in Berlin period. I suggest g...  5.0 of 5 bubbles   \n",
       "2     ['I found this place on TripAdvisor and wanted...  4.5 of 5 bubbles   \n",
       "3     [\"As my wife is a coeliac I had to research pl...  5.0 of 5 bubbles   \n",
       "4     ['This place is amaizing! I loved the vibe, th...  5.0 of 5 bubbles   \n",
       "...                                                 ...               ...   \n",
       "5785                                                NaN               NaN   \n",
       "5786                                                NaN               NaN   \n",
       "5787                                                NaN               NaN   \n",
       "5788                                                NaN               NaN   \n",
       "5789                                                NaN               NaN   \n",
       "\n",
       "      page number  data offset  restaurant  \\\n",
       "0               0            0           1   \n",
       "1               0            0           2   \n",
       "2               0            0           3   \n",
       "3               0            0           4   \n",
       "4               0            0           5   \n",
       "...           ...          ...         ...   \n",
       "5785          211         6330        6356   \n",
       "5786          211         6330        6357   \n",
       "5787          211         6330        6358   \n",
       "5788          211         6330        6359   \n",
       "5789          211         6330        6360   \n",
       "\n",
       "                                                  dates  \n",
       "0     ['13 November 2023', '23 September 2023', '28 ...  \n",
       "1     ['25 October 2023', '10 August 2023', '9 June ...  \n",
       "2     ['8 November 2023', '4 November 2023', '4 Nove...  \n",
       "3     ['24 October 2023', '1 October 2023', '15 Augu...  \n",
       "4     ['23 August 2023', '23 August 2023', '21 April...  \n",
       "...                                                 ...  \n",
       "5785                                                NaN  \n",
       "5786                                                NaN  \n",
       "5787                                                NaN  \n",
       "5788                                                NaN  \n",
       "5789                                                NaN  \n",
       "\n",
       "[5790 rows x 9 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "berlin_reviews_2    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3887145-8923-4088-b40c-18b3292ce4f7",
   "metadata": {},
   "source": [
    "### Is it useable tho?\n",
    "This Dataset has been updated 6 months ago! \n",
    "But seems like the reviews are still from late 2023 or older! \n",
    "Lets check if that's true!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b831bf28-3a27-4872-a10a-091066784219",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# convert each stringified list of date strings into actual datetime objects\n",
    "import ast\n",
    "\n",
    "# First tunring stringified lists to ACTUAL lists:\n",
    "berlin_reviews_2[\"dates\"] = berlin_reviews_2[\"dates\"].apply(\n",
    "    lambda x: ast.literal_eval(x) if isinstance(x, str) and x.startswith(\"[\") else x\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27146b34-8c90-431e-a7a6-641ee2466b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now getting lazy and using my preprocessing methods to turn lists of strings to list of dates\n",
    "from src.preprocessing import add_date_features\n",
    "berlin_reviews_2 = add_date_features(berlin_reviews_2, date_column=\"dates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b41a0f12-364c-4484-ba32-b992ee0c6436",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Timestamp('2023-11-13 00:00:00'),\n",
       " Timestamp('2023-09-23 00:00:00'),\n",
       " Timestamp('2023-08-28 00:00:00'),\n",
       " Timestamp('2023-08-11 00:00:00'),\n",
       " Timestamp('2023-08-08 00:00:00'),\n",
       " Timestamp('2023-08-01 00:00:00'),\n",
       " Timestamp('2023-07-01 00:00:00'),\n",
       " Timestamp('2023-06-10 00:00:00'),\n",
       " Timestamp('2023-05-14 00:00:00'),\n",
       " Timestamp('2023-05-10 00:00:00'),\n",
       " Timestamp('2023-04-11 00:00:00'),\n",
       " Timestamp('2023-01-31 00:00:00'),\n",
       " Timestamp('2023-01-13 00:00:00'),\n",
       " Timestamp('2022-12-28 00:00:00'),\n",
       " Timestamp('2022-12-04 00:00:00'),\n",
       " Timestamp('2022-06-22 00:00:00'),\n",
       " Timestamp('2022-05-16 00:00:00'),\n",
       " Timestamp('2022-05-11 00:00:00'),\n",
       " Timestamp('2022-02-10 00:00:00'),\n",
       " Timestamp('2022-02-09 00:00:00'),\n",
       " Timestamp('2021-08-11 00:00:00'),\n",
       " Timestamp('2020-09-03 00:00:00'),\n",
       " Timestamp('2020-07-24 00:00:00'),\n",
       " Timestamp('2020-01-03 00:00:00'),\n",
       " Timestamp('2019-12-26 00:00:00'),\n",
       " Timestamp('2019-10-09 00:00:00'),\n",
       " Timestamp('2019-10-01 00:00:00'),\n",
       " Timestamp('2019-09-28 00:00:00'),\n",
       " Timestamp('2019-06-14 00:00:00'),\n",
       " Timestamp('2019-05-03 00:00:00'),\n",
       " Timestamp('2019-04-15 00:00:00'),\n",
       " Timestamp('2019-04-15 00:00:00'),\n",
       " Timestamp('2019-03-28 00:00:00'),\n",
       " Timestamp('2019-03-18 00:00:00'),\n",
       " Timestamp('2019-03-13 00:00:00'),\n",
       " Timestamp('2018-12-05 00:00:00'),\n",
       " Timestamp('2018-11-22 00:00:00'),\n",
       " Timestamp('2018-11-21 00:00:00'),\n",
       " Timestamp('2018-10-27 00:00:00'),\n",
       " Timestamp('2018-10-26 00:00:00'),\n",
       " Timestamp('2018-10-23 00:00:00'),\n",
       " Timestamp('2018-10-23 00:00:00'),\n",
       " Timestamp('2018-10-23 00:00:00'),\n",
       " Timestamp('2018-10-05 00:00:00'),\n",
       " Timestamp('2018-09-24 00:00:00'),\n",
       " Timestamp('2018-09-05 00:00:00'),\n",
       " Timestamp('2018-09-04 00:00:00'),\n",
       " Timestamp('2018-09-04 00:00:00'),\n",
       " Timestamp('2018-08-21 00:00:00'),\n",
       " Timestamp('2018-08-13 00:00:00'),\n",
       " Timestamp('2018-07-04 00:00:00'),\n",
       " Timestamp('2018-06-12 00:00:00'),\n",
       " Timestamp('2018-06-04 00:00:00'),\n",
       " Timestamp('2018-05-19 00:00:00'),\n",
       " Timestamp('2018-05-19 00:00:00'),\n",
       " Timestamp('2018-05-18 00:00:00'),\n",
       " Timestamp('2018-05-09 00:00:00'),\n",
       " Timestamp('2018-05-03 00:00:00'),\n",
       " Timestamp('2018-05-02 00:00:00'),\n",
       " Timestamp('2018-05-02 00:00:00'),\n",
       " Timestamp('2018-05-01 00:00:00'),\n",
       " Timestamp('2018-05-01 00:00:00'),\n",
       " Timestamp('2018-04-30 00:00:00'),\n",
       " Timestamp('2018-03-15 00:00:00'),\n",
       " Timestamp('2018-03-11 00:00:00'),\n",
       " Timestamp('2018-03-03 00:00:00'),\n",
       " Timestamp('2018-01-22 00:00:00'),\n",
       " Timestamp('2018-01-22 00:00:00'),\n",
       " Timestamp('2018-01-15 00:00:00'),\n",
       " Timestamp('2017-12-19 00:00:00'),\n",
       " Timestamp('2017-12-14 00:00:00'),\n",
       " Timestamp('2017-12-06 00:00:00'),\n",
       " Timestamp('2017-11-19 00:00:00'),\n",
       " Timestamp('2017-11-17 00:00:00'),\n",
       " Timestamp('2017-11-12 00:00:00'),\n",
       " Timestamp('2017-10-22 00:00:00'),\n",
       " Timestamp('2017-10-21 00:00:00'),\n",
       " Timestamp('2017-10-06 00:00:00'),\n",
       " Timestamp('2017-09-27 00:00:00'),\n",
       " Timestamp('2017-08-28 00:00:00'),\n",
       " Timestamp('2017-08-23 00:00:00'),\n",
       " Timestamp('2017-08-13 00:00:00'),\n",
       " Timestamp('2017-06-18 00:00:00'),\n",
       " Timestamp('2017-06-07 00:00:00'),\n",
       " Timestamp('2017-05-18 00:00:00'),\n",
       " Timestamp('2017-05-08 00:00:00'),\n",
       " Timestamp('2017-04-23 00:00:00'),\n",
       " Timestamp('2017-04-23 00:00:00'),\n",
       " Timestamp('2017-04-15 00:00:00'),\n",
       " Timestamp('2017-04-14 00:00:00'),\n",
       " Timestamp('2017-04-12 00:00:00'),\n",
       " Timestamp('2017-04-11 00:00:00'),\n",
       " Timestamp('2017-04-09 00:00:00'),\n",
       " Timestamp('2017-04-03 00:00:00'),\n",
       " Timestamp('2017-03-29 00:00:00'),\n",
       " Timestamp('2017-03-23 00:00:00'),\n",
       " Timestamp('2017-03-08 00:00:00'),\n",
       " Timestamp('2017-02-21 00:00:00'),\n",
       " Timestamp('2017-02-19 00:00:00'),\n",
       " Timestamp('2017-02-15 00:00:00'),\n",
       " Timestamp('2017-01-28 00:00:00'),\n",
       " Timestamp('2017-01-25 00:00:00'),\n",
       " Timestamp('2017-01-22 00:00:00'),\n",
       " Timestamp('2017-01-19 00:00:00'),\n",
       " Timestamp('2017-01-09 00:00:00'),\n",
       " Timestamp('2016-12-20 00:00:00'),\n",
       " Timestamp('2016-12-13 00:00:00'),\n",
       " Timestamp('2016-12-11 00:00:00'),\n",
       " Timestamp('2016-12-05 00:00:00'),\n",
       " Timestamp('2016-12-01 00:00:00'),\n",
       " Timestamp('2016-11-22 00:00:00'),\n",
       " Timestamp('2016-11-18 00:00:00'),\n",
       " Timestamp('2016-11-16 00:00:00'),\n",
       " Timestamp('2016-11-12 00:00:00'),\n",
       " Timestamp('2016-11-02 00:00:00'),\n",
       " Timestamp('2016-11-01 00:00:00'),\n",
       " Timestamp('2016-10-29 00:00:00'),\n",
       " Timestamp('2016-10-27 00:00:00'),\n",
       " Timestamp('2016-10-23 00:00:00'),\n",
       " Timestamp('2016-10-22 00:00:00'),\n",
       " Timestamp('2016-10-20 00:00:00'),\n",
       " Timestamp('2016-10-07 00:00:00'),\n",
       " Timestamp('2016-09-29 00:00:00'),\n",
       " Timestamp('2016-09-28 00:00:00'),\n",
       " Timestamp('2016-09-22 00:00:00'),\n",
       " Timestamp('2016-09-13 00:00:00'),\n",
       " Timestamp('2016-08-11 00:00:00'),\n",
       " Timestamp('2016-07-29 00:00:00'),\n",
       " Timestamp('2016-07-28 00:00:00'),\n",
       " Timestamp('2016-07-25 00:00:00'),\n",
       " Timestamp('2016-07-25 00:00:00'),\n",
       " Timestamp('2016-07-22 00:00:00'),\n",
       " Timestamp('2016-07-21 00:00:00'),\n",
       " Timestamp('2016-07-21 00:00:00'),\n",
       " Timestamp('2016-07-20 00:00:00'),\n",
       " Timestamp('2016-07-20 00:00:00'),\n",
       " Timestamp('2016-07-18 00:00:00'),\n",
       " Timestamp('2016-07-15 00:00:00'),\n",
       " Timestamp('2016-07-14 00:00:00'),\n",
       " Timestamp('2016-07-13 00:00:00'),\n",
       " Timestamp('2016-07-11 00:00:00'),\n",
       " Timestamp('2016-07-02 00:00:00'),\n",
       " Timestamp('2016-06-28 00:00:00'),\n",
       " Timestamp('2016-06-26 00:00:00'),\n",
       " Timestamp('2016-06-21 00:00:00'),\n",
       " Timestamp('2016-06-15 00:00:00'),\n",
       " Timestamp('2016-05-29 00:00:00'),\n",
       " Timestamp('2016-05-29 00:00:00'),\n",
       " Timestamp('2016-05-25 00:00:00'),\n",
       " Timestamp('2016-05-24 00:00:00'),\n",
       " Timestamp('2016-05-23 00:00:00'),\n",
       " Timestamp('2016-05-22 00:00:00'),\n",
       " Timestamp('2016-05-21 00:00:00'),\n",
       " Timestamp('2016-05-18 00:00:00'),\n",
       " Timestamp('2016-05-18 00:00:00'),\n",
       " Timestamp('2016-05-17 00:00:00'),\n",
       " Timestamp('2016-04-29 00:00:00'),\n",
       " Timestamp('2016-04-27 00:00:00'),\n",
       " Timestamp('2016-04-26 00:00:00'),\n",
       " Timestamp('2016-03-30 00:00:00'),\n",
       " Timestamp('2016-03-27 00:00:00'),\n",
       " Timestamp('2016-03-06 00:00:00'),\n",
       " Timestamp('2016-03-05 00:00:00'),\n",
       " Timestamp('2016-02-17 00:00:00'),\n",
       " Timestamp('2016-02-09 00:00:00'),\n",
       " Timestamp('2015-11-07 00:00:00'),\n",
       " Timestamp('2015-10-30 00:00:00'),\n",
       " Timestamp('2015-10-29 00:00:00'),\n",
       " Timestamp('2015-10-19 00:00:00'),\n",
       " Timestamp('2015-10-13 00:00:00'),\n",
       " Timestamp('2015-10-13 00:00:00'),\n",
       " Timestamp('2015-10-10 00:00:00'),\n",
       " Timestamp('2015-10-08 00:00:00'),\n",
       " Timestamp('2015-09-30 00:00:00'),\n",
       " Timestamp('2015-09-29 00:00:00'),\n",
       " Timestamp('2015-09-22 00:00:00'),\n",
       " Timestamp('2015-09-20 00:00:00'),\n",
       " Timestamp('2015-09-16 00:00:00'),\n",
       " Timestamp('2015-09-13 00:00:00'),\n",
       " Timestamp('2015-09-12 00:00:00'),\n",
       " Timestamp('2015-09-11 00:00:00'),\n",
       " Timestamp('2015-09-06 00:00:00'),\n",
       " Timestamp('2015-08-31 00:00:00'),\n",
       " Timestamp('2015-08-24 00:00:00'),\n",
       " Timestamp('2015-08-22 00:00:00'),\n",
       " Timestamp('2015-08-20 00:00:00'),\n",
       " Timestamp('2015-08-19 00:00:00'),\n",
       " Timestamp('2015-08-15 00:00:00'),\n",
       " Timestamp('2015-08-13 00:00:00'),\n",
       " Timestamp('2015-07-19 00:00:00'),\n",
       " Timestamp('2015-07-14 00:00:00'),\n",
       " Timestamp('2015-07-04 00:00:00'),\n",
       " Timestamp('2015-06-15 00:00:00'),\n",
       " Timestamp('2015-06-10 00:00:00'),\n",
       " Timestamp('2015-05-25 00:00:00'),\n",
       " Timestamp('2015-04-24 00:00:00'),\n",
       " Timestamp('2015-03-27 00:00:00'),\n",
       " Timestamp('2015-03-21 00:00:00'),\n",
       " Timestamp('2015-03-05 00:00:00'),\n",
       " Timestamp('2015-02-28 00:00:00'),\n",
       " Timestamp('2015-02-25 00:00:00'),\n",
       " Timestamp('2015-02-22 00:00:00'),\n",
       " Timestamp('2015-02-11 00:00:00'),\n",
       " Timestamp('2015-01-30 00:00:00'),\n",
       " Timestamp('2014-12-05 00:00:00'),\n",
       " Timestamp('2014-09-29 00:00:00'),\n",
       " Timestamp('2014-09-26 00:00:00'),\n",
       " Timestamp('2014-09-10 00:00:00'),\n",
       " Timestamp('2014-06-23 00:00:00'),\n",
       " Timestamp('2013-12-12 00:00:00'),\n",
       " Timestamp('2013-10-11 00:00:00'),\n",
       " Timestamp('2013-07-30 00:00:00'),\n",
       " Timestamp('2012-07-12 00:00:00'),\n",
       " Timestamp('2012-05-28 00:00:00'),\n",
       " Timestamp('2012-02-13 00:00:00'),\n",
       " Timestamp('2012-01-14 00:00:00'),\n",
       " Timestamp('2011-08-19 00:00:00'),\n",
       " Timestamp('2023-11-13 00:00:00'),\n",
       " Timestamp('2023-09-23 00:00:00'),\n",
       " Timestamp('2023-08-28 00:00:00'),\n",
       " Timestamp('2023-08-11 00:00:00'),\n",
       " Timestamp('2023-08-08 00:00:00'),\n",
       " Timestamp('2023-08-01 00:00:00'),\n",
       " Timestamp('2023-07-01 00:00:00'),\n",
       " Timestamp('2023-06-10 00:00:00'),\n",
       " Timestamp('2023-05-14 00:00:00'),\n",
       " Timestamp('2023-05-10 00:00:00'),\n",
       " Timestamp('2023-04-11 00:00:00'),\n",
       " Timestamp('2023-01-31 00:00:00'),\n",
       " Timestamp('2023-01-13 00:00:00'),\n",
       " Timestamp('2022-12-28 00:00:00'),\n",
       " Timestamp('2022-12-04 00:00:00')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "berlin_reviews_2[\"dates_parsed\"].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ca35cd-1211-437a-9c1f-3733edf5e14c",
   "metadata": {},
   "source": [
    "Look above, weird behavior is noticed.. the first dates match the last dates... looks like some dates repeated... why?!\n",
    "Let's check some other rows!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cad1dded-dff7-4e5f-88cc-12b27352d5b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Timestamp('2023-10-25 00:00:00'),\n",
       " Timestamp('2023-08-10 00:00:00'),\n",
       " Timestamp('2023-06-09 00:00:00'),\n",
       " Timestamp('2023-04-18 00:00:00'),\n",
       " Timestamp('2023-03-12 00:00:00'),\n",
       " Timestamp('2023-02-10 00:00:00'),\n",
       " Timestamp('2023-02-07 00:00:00'),\n",
       " Timestamp('2023-01-21 00:00:00'),\n",
       " Timestamp('2023-01-06 00:00:00'),\n",
       " Timestamp('2022-12-10 00:00:00'),\n",
       " Timestamp('2022-09-14 00:00:00'),\n",
       " Timestamp('2022-06-16 00:00:00'),\n",
       " Timestamp('2022-06-03 00:00:00'),\n",
       " Timestamp('2022-03-04 00:00:00'),\n",
       " Timestamp('2021-06-24 00:00:00'),\n",
       " Timestamp('2021-04-17 00:00:00'),\n",
       " Timestamp('2020-07-31 00:00:00'),\n",
       " Timestamp('2019-12-21 00:00:00'),\n",
       " Timestamp('2019-08-21 00:00:00'),\n",
       " Timestamp('2019-08-20 00:00:00'),\n",
       " Timestamp('2019-08-14 00:00:00'),\n",
       " Timestamp('2019-07-04 00:00:00'),\n",
       " Timestamp('2019-06-14 00:00:00'),\n",
       " Timestamp('2019-06-13 00:00:00'),\n",
       " Timestamp('2019-06-07 00:00:00'),\n",
       " Timestamp('2019-06-04 00:00:00'),\n",
       " Timestamp('2019-06-01 00:00:00'),\n",
       " Timestamp('2019-05-24 00:00:00'),\n",
       " Timestamp('2019-05-11 00:00:00'),\n",
       " Timestamp('2019-01-31 00:00:00'),\n",
       " Timestamp('2019-01-08 00:00:00'),\n",
       " Timestamp('2018-12-05 00:00:00'),\n",
       " Timestamp('2018-11-29 00:00:00'),\n",
       " Timestamp('2018-11-16 00:00:00'),\n",
       " Timestamp('2018-11-09 00:00:00'),\n",
       " Timestamp('2018-11-09 00:00:00'),\n",
       " Timestamp('2018-10-24 00:00:00'),\n",
       " Timestamp('2018-10-24 00:00:00'),\n",
       " Timestamp('2018-10-18 00:00:00'),\n",
       " Timestamp('2018-10-15 00:00:00'),\n",
       " Timestamp('2018-08-25 00:00:00'),\n",
       " Timestamp('2018-07-25 00:00:00'),\n",
       " Timestamp('2018-04-07 00:00:00'),\n",
       " Timestamp('2017-07-15 00:00:00'),\n",
       " Timestamp('2016-09-05 00:00:00'),\n",
       " Timestamp('2016-05-29 00:00:00'),\n",
       " Timestamp('2016-03-06 00:00:00'),\n",
       " Timestamp('2023-10-25 00:00:00'),\n",
       " Timestamp('2023-08-10 00:00:00'),\n",
       " Timestamp('2023-06-09 00:00:00'),\n",
       " Timestamp('2023-04-18 00:00:00'),\n",
       " Timestamp('2023-03-12 00:00:00'),\n",
       " Timestamp('2023-02-10 00:00:00'),\n",
       " Timestamp('2023-02-07 00:00:00'),\n",
       " Timestamp('2023-01-21 00:00:00'),\n",
       " Timestamp('2023-01-06 00:00:00'),\n",
       " Timestamp('2022-12-10 00:00:00'),\n",
       " Timestamp('2022-09-14 00:00:00'),\n",
       " Timestamp('2022-06-16 00:00:00'),\n",
       " Timestamp('2022-06-03 00:00:00'),\n",
       " Timestamp('2022-03-04 00:00:00'),\n",
       " Timestamp('2021-06-24 00:00:00')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "berlin_reviews_2[\"dates_parsed\"].iloc[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32107f77-fc28-456b-bb01-a3d691934524",
   "metadata": {},
   "source": [
    "AGAIN!! We have EXACTLY 15 first dates matching the 15 last dates... that's a problem! Have i caused it? or the dataset has been like that since the beginning??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b26b528-168f-4047-adb6-e579e517646f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index nr = 3532\n",
      "[Timestamp('2018-08-08 00:00:00'), Timestamp('2015-09-13 00:00:00'), Timestamp('2015-08-04 00:00:00'), Timestamp('2015-06-25 00:00:00'), Timestamp('2015-04-20 00:00:00'), Timestamp('2012-09-23 00:00:00'), Timestamp('2018-08-08 00:00:00'), Timestamp('2015-09-13 00:00:00'), Timestamp('2015-08-04 00:00:00'), Timestamp('2015-06-25 00:00:00'), Timestamp('2015-04-20 00:00:00'), Timestamp('2012-09-23 00:00:00')]\n"
     ]
    }
   ],
   "source": [
    "# i check some random rows, to see if the exact 15 dates match, then i check the review! if the reviews are the same... then we need cleaning!!\n",
    "random_row = np.random.randint(0, 5000)\n",
    "print('index nr =', random_row)\n",
    "print(berlin_reviews_2[\"dates_parsed\"].iloc[random_row])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095c6e05-92a0-4a8f-b85e-fe869e6662f6",
   "metadata": {},
   "source": [
    "Some useful insights from the above algorythm:\n",
    "\n",
    "index nr = 4205\n",
    "\n",
    "[Timestamp('2023-09-27 00:00:00'), Timestamp('2023-09-27 00:00:00')]\n",
    "\n",
    "index nr = 2227\n",
    "\n",
    "['3 June 2023', '30 December 2019', '15 August 2019', '1 June 2019', '6 May 2019', '3 June 2023', '30 December 2019', '15 August 2019', '1 June 2019', '6 May 2019']\n",
    "\n",
    "index nr = 4675\n",
    "\n",
    "['28 May 2023', '28 May 2023']\n",
    "\n",
    "\n",
    "So now we have another challenge, when the nr of reviews and therefore the date of reviews are less than 15 in tottal!! \n",
    "### ACTION: Cleaning later needed!\n",
    "### Cleaning 1: Done [ ]  or  Due [ ]\n",
    "DO THIS: when less than 15 review dates, clean the last half! EZ PZ!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3cdd98f7-8e61-47ac-9c4b-d40dfba46a98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>earliest_date</th>\n",
       "      <th>latest_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011-08-19</td>\n",
       "      <td>2023-11-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-03-06</td>\n",
       "      <td>2023-10-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2008-01-19</td>\n",
       "      <td>2023-11-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-05-08</td>\n",
       "      <td>2023-10-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-05-07</td>\n",
       "      <td>2023-08-23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  earliest_date latest_date\n",
       "0    2011-08-19  2023-11-13\n",
       "1    2016-03-06  2023-10-25\n",
       "2    2008-01-19  2023-11-08\n",
       "3    2017-05-08  2023-10-24\n",
       "4    2019-05-07  2023-08-23"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now back to extracting earliest and latest review to see if the dataset is up to date as kaggle shows:\n",
    "berlin_reviews_2[[\"earliest_date\", \"latest_date\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "89d18179-4244-40a5-b154-f1a3b5f0f959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2007-11-01 00:00:00\n",
      "2023-11-20 00:00:00\n"
     ]
    }
   ],
   "source": [
    "print(berlin_reviews_2[\"earliest_date\"].min())\n",
    "print(berlin_reviews_2[\"latest_date\"].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c87d53-0f20-43b3-88e1-3cee4d1cfb59",
   "metadata": {},
   "source": [
    "### Summary of Dataset berlin_reviews_2:\n",
    "**1st Summary:**\n",
    "The author has updated this dataset 6 months ago on Kaggle, but still the last review goes back to 2023.\n",
    "In all honesty, just exploring random data on this data set, i could tell the latest reviews are mostly from 2023 and then 2022 and so on. so it def is not too old to maneuver with! But a more updated dataset is appreciated. Off the next dataset now!\n",
    "\n",
    "Newest Review:2023-11-20\n",
    "\n",
    "Oldest Review: 2007-11-01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757b56a7-ead4-43df-8b06-3e5ef59cd309",
   "metadata": {},
   "source": [
    "## 2. Foursquare API for berlin venues:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84381a8d-402a-4c2b-8069-4e44adaefabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @hidden_cell\n",
    "\n",
    "CLIENT_ID = 'HIE5ISYEZQEQPNSVGQGRWCJMXA43OC3MBRHICDU01GF1P0EA' # your Foursquare ID\n",
    "CLIENT_SECRET = 'DK3E0ME2RXUUXAOX54VSSULBVYBUJWUD4BRVAQIJMV2ZIG54' # your Foursquare Secret\n",
    "VERSION = '20190701' # Foursquare API version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "339e576e-1495-49ed-b2fd-55628570fe88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns a DataFrame with Venue details\n",
    "def getNearbyVenues(names, latitudes, longitudes, radius, LIMIT):\n",
    "    \n",
    "    venues_list=[]\n",
    "    for name, lat, lng in zip(names, latitudes, longitudes):\n",
    "        # create the API request URL\n",
    "        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(\n",
    "            CLIENT_ID, \n",
    "            CLIENT_SECRET, \n",
    "            VERSION, \n",
    "            lat, \n",
    "            lng, \n",
    "            radius, \n",
    "            LIMIT)\n",
    "            \n",
    "        # make the GET request\n",
    "        results = requests.get(url).json()[\"response\"]['groups'][0]['items']\n",
    "        # return only relevant information for each nearby venue\n",
    "        venues_list.append([(\n",
    "            name, \n",
    "            lat, \n",
    "            lng, \n",
    "            v['venue']['name'], \n",
    "            v['venue']['location']['lat'], \n",
    "            v['venue']['location']['lng'],  \n",
    "            v['venue']['categories'][0]['name']) for v in results])\n",
    "\n",
    "    nearby_venues = pd.DataFrame([item for venue_list in venues_list for item in venue_list])\n",
    "    nearby_venues.columns = ['Neighbourhood', \n",
    "                  'Neighbourhood Latitude', \n",
    "                  'Neighbourhood Longitude', \n",
    "                  'Venue', \n",
    "                  'Venue Latitude', \n",
    "                  'Venue Longitude', \n",
    "                  'Venue Category']\n",
    "    \n",
    "    return(nearby_venues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a7a4998-3c02-402c-ac0b-6735a3af6e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns a DataFrame with top venues based on a threshold\n",
    "def return_top_venues(row, maximum_venues):\n",
    "    \n",
    "    # Select all except neighbourhood column\n",
    "    row_categories = row.iloc[1:]\n",
    "    row_categories_sorted = row_categories.sort_values(ascending=False)\n",
    "    \n",
    "    return row_categories_sorted.index.values[0: maximum_venues]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27d5a820-cd4b-4cfa-bf41-4cb713c957de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching data, could be caused by exceeding Foursquare maximum calls/ day.\n",
      "Cannot view data: Berlin_venues DataFrame was not successfully created.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "# Try to fetch Berlin Venues data\n",
    "try:\n",
    "    Berlin_venues = getNearbyVenues(Berlin_neighbourhoods['Neighbourhood'],\n",
    "                                        Berlin_neighbourhoods['Latitude'],\n",
    "                                        Berlin_neighbourhoods['Longitude'],\n",
    "                                        radius=2000,\n",
    "                                        LIMIT=100)\n",
    "except:\n",
    "    print('Error fetching data, could be caused by exceeding Forsquare maximum calls/ day.')\n",
    "\n",
    "# View the DataFrame\n",
    "Berlin_venues.head()\n",
    "'''\n",
    "# 1. Initialize the variable outside the try block\n",
    "Berlin_venues = None\n",
    "\n",
    "# 2. Try to fetch Berlin Venues data\n",
    "try:\n",
    "    Berlin_venues = getNearbyVenues(Berlin_neighbourhoods['Neighbourhood'],\n",
    "                                    Berlin_neighbourhoods['Latitude'],\n",
    "                                    Berlin_neighbourhoods['Longitude'],\n",
    "                                    radius=2000,\n",
    "                                    LIMIT=100)\n",
    "except Exception as e: # Catch the specific exception for better debugging\n",
    "    print('Error fetching data, could be caused by exceeding Foursquare maximum calls/ day.')\n",
    "    # print(f\"Details: {e}\") # Uncomment to see the actual error details\n",
    "\n",
    "# 3. View the DataFrame ONLY IF it was successfully created\n",
    "if Berlin_venues is not None:\n",
    "    Berlin_venues.head()\n",
    "else:\n",
    "    print(\"Cannot view data: Berlin_venues DataFrame was not successfully created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c6e881b-f643-4425-a204-12d8dd7851f6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Berlin_neighbourhoods' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m Berlin_venues \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# 2. Slice the DataFrame to use only the first 5 neighborhoods\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m sample_neighbourhoods \u001b[38;5;241m=\u001b[39m \u001b[43mBerlin_neighbourhoods\u001b[49m\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâœ… Attempting to fetch venues for the first \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(sample_neighbourhoods)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m neighborhoods...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# 3. Try to fetch Berlin Venues data using the smaller sample\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Berlin_neighbourhoods' is not defined"
     ]
    }
   ],
   "source": [
    "# 1. Initialize the variable outside the try block\n",
    "Berlin_venues = None\n",
    "\n",
    "# 2. Slice the DataFrame to use only the first 5 neighborhoods\n",
    "sample_neighbourhoods = Berlin_neighbourhoods.head(5)\n",
    "\n",
    "print(f\"âœ… Attempting to fetch venues for the first {len(sample_neighbourhoods)} neighborhoods...\")\n",
    "\n",
    "# 3. Try to fetch Berlin Venues data using the smaller sample\n",
    "try:\n",
    "    Berlin_venues = getNearbyVenues(sample_neighbourhoods['Neighbourhood'],\n",
    "                                    sample_neighbourhoods['Latitude'],\n",
    "                                    sample_neighbourhoods['Longitude'],\n",
    "                                    radius=2000,\n",
    "                                    LIMIT=100)\n",
    "except Exception as e:\n",
    "    print('âŒ Error fetching data, could be caused by exceeding Foursquare maximum calls/ day or an issue with the API keys.')\n",
    "    # print(f\"Details: {e}\") # Uncomment to see the actual error details\n",
    "\n",
    "# 4. View the DataFrame ONLY IF it was successfully created\n",
    "if Berlin_venues is not None:\n",
    "    print(\"\\nðŸŽ‰ Success! Displaying the first 5 rows of the fetched data:\")\n",
    "    Berlin_venues.head()\n",
    "else:\n",
    "    print(\"\\nâš ï¸ Cannot view data: Berlin_venues DataFrame was not successfully created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e9b9a149-5c3e-4e81-9176-305652e5568a",
   "metadata": {},
   "outputs": [
    {
     "ename": "FeatureNotFound",
     "evalue": "Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFeatureNotFound\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Get HTML file of Berlin Neighbourhoods\u001b[39;00m\n\u001b[1;32m      2\u001b[0m Berlin_neighbourhoods_html \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://en.wikipedia.org/wiki/Boroughs_and_neighborhoods_of_Berlin\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mtext\n\u001b[0;32m----> 3\u001b[0m page \u001b[38;5;241m=\u001b[39m \u001b[43mBeautifulSoup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBerlin_neighbourhoods_html\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlxml\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Check Webpage Title\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1. Webpage Title: \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m----------------------\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(page\u001b[38;5;241m.\u001b[39mtitle\u001b[38;5;241m.\u001b[39mtext))\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/berlin_bites/lib/python3.10/site-packages/bs4/__init__.py:366\u001b[0m, in \u001b[0;36mBeautifulSoup.__init__\u001b[0;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[0m\n\u001b[1;32m    364\u001b[0m     possible_builder_class \u001b[38;5;241m=\u001b[39m builder_registry\u001b[38;5;241m.\u001b[39mlookup(\u001b[38;5;241m*\u001b[39mfeatures)\n\u001b[1;32m    365\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m possible_builder_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 366\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m FeatureNotFound(\n\u001b[1;32m    367\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find a tree builder with the features you \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    368\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequested: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m. Do you need to install a parser library?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    369\u001b[0m             \u001b[38;5;241m%\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(features)\n\u001b[1;32m    370\u001b[0m         )\n\u001b[1;32m    371\u001b[0m     builder_class \u001b[38;5;241m=\u001b[39m possible_builder_class\n\u001b[1;32m    373\u001b[0m \u001b[38;5;66;03m# At this point either we have a TreeBuilder instance in\u001b[39;00m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;66;03m# builder, or we have a builder_class that we can instantiate\u001b[39;00m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;66;03m# with the remaining **kwargs.\u001b[39;00m\n",
      "\u001b[0;31mFeatureNotFound\u001b[0m: Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?"
     ]
    }
   ],
   "source": [
    "# Get HTML file of Berlin Neighbourhoods\n",
    "Berlin_neighbourhoods_html = requests.get('https://en.wikipedia.org/wiki/Boroughs_and_neighborhoods_of_Berlin').text\n",
    "page = BeautifulSoup(Berlin_neighbourhoods_html, 'lxml')\n",
    "\n",
    "# Check Webpage Title\n",
    "print('1. Webpage Title: \\n----------------------\\n{} \\n \\n'.format(page.title.text))\n",
    "\n",
    "# Find the first table in the webpage\n",
    "table = page.find('table')\n",
    "\n",
    "# Create a DataFrame for Boroughs in Berlin\n",
    "Berlin_boroughs = pd.DataFrame()\n",
    "\n",
    "# Assign the retrived table to a list\n",
    "data_list = pd.read_html(str(table))\n",
    "\n",
    "# Copy retrived data to the Berlin_boroughs DataFrame\n",
    "Berlin_boroughs = data_list[0]\n",
    "\n",
    "# Rename Columns and drop the Map column\n",
    "Berlin_boroughs.drop(['Map'], axis =1, inplace=True)\n",
    "Berlin_boroughs.columns = ['Boroughs', 'Population', 'Area', 'Density']\n",
    "\n",
    "print('2. DataFrame Shape:\\n----------------------\\n', Berlin_boroughs.shape,'\\n \\n')\n",
    "Berlin_boroughs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686f05b3-a8cb-4046-9ccb-5bbda824ba6c",
   "metadata": {},
   "source": [
    "### **For NOW** i hold it with this FSQ data source! \n",
    "### It is more useful for geografical insights than reviews!\n",
    "But i have found some other veryy verryyy intersting data sources for super advanced data insights.\n",
    "\n",
    "These are as followed:\n",
    "\n",
    "https://kepler.gl/\n",
    "\n",
    "https://deck.gl/\n",
    "\n",
    "https://opensource.foursquare.com/os-places/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503c8327-25dd-4ab5-8e57-aa66c840967b",
   "metadata": {},
   "source": [
    "## 3. yelp: https://www.youtube.com/watch?v=mn6aj3JitVo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "79019ce9-6f8e-4188-81d4-4dabe1847a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_yelp_reviews(api_key, location='Berlin', term='restaurant', limit=50):\n",
    "    headers = {'Authorization': f'Bearer {api_key}'}\n",
    "    url = 'https://api.yelp.com/v3/businesses/search'\n",
    "    params = {'location': location, 'term': term, 'limit': limit}\n",
    "    response = requests.get(url, headers=headers, params=params).json()\n",
    "    \n",
    "    reviews_list = []\n",
    "    for business in response['businesses']:\n",
    "        reviews_list.append({\n",
    "            'name': business['name'],\n",
    "            'rating': business['rating'],\n",
    "            'review_count': business['review_count'],\n",
    "            'category': business['categories'][0]['title']\n",
    "        })\n",
    "    return pd.DataFrame(reviews_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a47bb3-a871-41d6-b669-d146c2e78960",
   "metadata": {},
   "source": [
    "ok so yelp only gives me 3 reviews per place... that's... not ideal for this project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572a3f4f-2db1-45e9-8fb1-eee93769e433",
   "metadata": {},
   "outputs": [],
   "source": [
    "HEADERS = {\n",
    "    \"User-Agent\":\n",
    "        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) \"\n",
    "        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "        \"Chrome/124.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# SEARCH FOR RESTAURANTS (YELP HTML, NOT API)\n",
    "# ============================================================\n",
    "\n",
    "def search_yelp_berlin(term=\"restaurants\", pages=3) -> List[str]:\n",
    "    \"\"\"\n",
    "    Scrape Yelp search results for Berlin.\n",
    "    Returns a list of restaurant URLs.\n",
    "\n",
    "    Parameters:\n",
    "        term: search query (\"restaurants\", \"cafes\", \"pizza\", ...)\n",
    "        pages: how many result pages to scrape (15â€“20 restaurants per page)\n",
    "\n",
    "    Returns:\n",
    "        list of restaurant page URLs\n",
    "    \"\"\"\n",
    "    base_url = \"https://www.yelp.com/search\"\n",
    "    urls = []\n",
    "\n",
    "    for p in range(pages):\n",
    "        params = {\"find_desc\": term, \"find_loc\": \"Berlin\", \"start\": p * 10}\n",
    "        print(f\"[INFO] Fetching search page {p+1}...\")\n",
    "\n",
    "        res = requests.get(base_url, headers=HEADERS, params=params)\n",
    "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "\n",
    "        # Yelp URLs look like this: /biz/some-restaurant-berlin\n",
    "        for link in soup.select(\"a.css-1k2tj2c\"):\n",
    "            href = link.get(\"href\", \"\")\n",
    "            if href.startswith(\"/biz/\") and \"?\" not in href:\n",
    "                full_url = \"https://www.yelp.com\" + href\n",
    "                urls.append(full_url)\n",
    "\n",
    "        time.sleep(random.uniform(1, 2))\n",
    "\n",
    "    urls = list(dict.fromkeys(urls))  # deduplicate\n",
    "    print(f\"[INFO] Found {len(urls)} restaurant URLs.\")\n",
    "    return urls\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# PARSE INDIVIDUAL REVIEW BLOCK\n",
    "# ============================================================\n",
    "\n",
    "def parse_review_block(block) -> Dict:\n",
    "    \"\"\"\n",
    "    Extracts review info from one review HTML block.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        text = block.select_one(\"span.raw__09f24__T4Ezm\").get_text(strip=True)\n",
    "    except:\n",
    "        text = \"\"\n",
    "\n",
    "    try:\n",
    "        rating_tag = block.select_one(\"div.i-stars__09f24__M1AR7\")\n",
    "        rating = float(rating_tag[\"aria-label\"].split()[0])\n",
    "    except:\n",
    "        rating = None\n",
    "\n",
    "    try:\n",
    "        date = block.select_one(\"span.css-1e4fdj9\").get_text(strip=True)\n",
    "    except:\n",
    "        date = None\n",
    "\n",
    "    return {\n",
    "        \"review_text\": text,\n",
    "        \"review_rating\": rating,\n",
    "        \"review_date\": date\n",
    "    }\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# SCRAPE ALL REVIEWS FROM A RESTAURANT PAGE\n",
    "# ============================================================\n",
    "\n",
    "def scrape_restaurant_reviews(url: str, max_pages=20) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Scrape multiple pages of Yelp reviews for a restaurant.\n",
    "\n",
    "    Parameters:\n",
    "        url: Yelp restaurant URL (e.g. \"https://www.yelp.com/biz/...\").\n",
    "        max_pages: max number of pagination pages to follow.\n",
    "\n",
    "    Returns:\n",
    "        list of review dicts\n",
    "    \"\"\"\n",
    "    print(f\"[SCRAPE] {url}\")\n",
    "    reviews = []\n",
    "\n",
    "    for p in range(max_pages):\n",
    "        paged_url = f\"{url}?start={p * 10}\"\n",
    "\n",
    "        res = requests.get(paged_url, headers=HEADERS)\n",
    "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "\n",
    "        review_blocks = soup.select(\"div.review__09f24__oHr9V\")\n",
    "\n",
    "        if not review_blocks:\n",
    "            break  # no more pages\n",
    "\n",
    "        for block in review_blocks:\n",
    "            rev = parse_review_block(block)\n",
    "            rev[\"restaurant_url\"] = url\n",
    "            reviews.append(rev)\n",
    "\n",
    "        time.sleep(random.uniform(1.0, 2.0))\n",
    "\n",
    "    print(f\"[DONE] {len(reviews)} reviews scraped.\")\n",
    "    return reviews\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# SCRAPE ALL RESTAURANTS + SAVE CLEAN DATASET\n",
    "# ============================================================\n",
    "\n",
    "def collect_berlin_reviews(term=\"restaurants\", pages=3, max_review_pages=20):\n",
    "    \"\"\"\n",
    "    Main pipeline:\n",
    "    1) Find restaurant URLs through Yelp search\n",
    "    2) Scrape all reviews from each restaurant\n",
    "    3) Save raw + clean CSVs\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with all reviews\n",
    "    \"\"\"\n",
    "    urls = search_yelp_berlin(term=term, pages=pages)\n",
    "    all_reviews = []\n",
    "\n",
    "    for url in urls:\n",
    "        try:\n",
    "            reviews = scrape_restaurant_reviews(url, max_pages=max_review_pages)\n",
    "            all_reviews.extend(reviews)\n",
    "        except Exception as e:\n",
    "            print(f\"[ERR] Failed scraping {url}: {e}\")\n",
    "            continue\n",
    "\n",
    "    df = pd.DataFrame(all_reviews)\n",
    "\n",
    "    df[\"crawled_at\"] = datetime.utcnow().isoformat()\n",
    "\n",
    "    df.to_csv(\"data/raw/yelp_berlin_reviews_raw.csv\", index=False)\n",
    "    print(\"[SAVE] Raw dataset saved to data/raw/yelp_berlin_reviews_raw.csv\")\n",
    "\n",
    "    return df\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
