{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28da06f7-d4f7-4411-96ac-3cb276cc17da",
   "metadata": {},
   "source": [
    "# Exploring data about Belrin Restaurants and the respected Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3560bb65-896a-4096-b909-8d732d1df757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMPORT STATUS:\t DONE.\n"
     ]
    }
   ],
   "source": [
    "# Import libraries and packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import csv\n",
    "import sys, os\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "sys.path.append(project_root)\n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "# Machine Learning and Plotting\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Library to handle JSON files\n",
    "import json\n",
    "import requests \n",
    "\n",
    "# Machine Learning and Plotting\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# NOT IN THE DEPENDENCIES RN!! \n",
    "\n",
    "# Maps visualization:\n",
    "# import folium\n",
    "# Address to geographical data:\n",
    "#from geopy.geocoders import Nominatim\n",
    "\n",
    "# Web Scraping and Reading HTML files\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Used with lists\n",
    "import itertools\n",
    "\n",
    "# To add delay between quries\n",
    "import time\n",
    "\n",
    "# To Ignore Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(\"IMPORT STATUS:\\t DONE.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b8f7aa-4124-4486-9f4e-bdacff49a041",
   "metadata": {},
   "source": [
    "## 1. Exploring berlin_reviews_2 csv dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2282eaf1-3665-43fd-befd-0e48caba6105",
   "metadata": {},
   "outputs": [],
   "source": [
    "berlin_reviews_2 = pd.read_csv('../data/raw/berlin_reviews_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4a6aba1-28ec-4edc-b40d-4ae6faf4ec0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>cuisines</th>\n",
       "      <th>reviews</th>\n",
       "      <th>star rating</th>\n",
       "      <th>page number</th>\n",
       "      <th>data offset</th>\n",
       "      <th>restaurant</th>\n",
       "      <th>dates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1. Focaccino</td>\n",
       "      <td>https://www.tripadvisor.in/Restaurant_Review-g...</td>\n",
       "      <td>Italian, Sicilian</td>\n",
       "      <td>[\"The atmosphere is very classy yet cozy. The ...</td>\n",
       "      <td>4.5 of 5 bubbles</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>['13 November 2023', '23 September 2023', '28 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2. Cafe Couscous - Vege</td>\n",
       "      <td>https://www.tripadvisor.in/Restaurant_Review-g...</td>\n",
       "      <td>Healthy, Mediterranean</td>\n",
       "      <td>[\"The best wraps in Berlin period. I suggest g...</td>\n",
       "      <td>5.0 of 5 bubbles</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>['25 October 2023', '10 August 2023', '9 June ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3. Hackethals</td>\n",
       "      <td>https://www.tripadvisor.in/Restaurant_Review-g...</td>\n",
       "      <td>German, Bar</td>\n",
       "      <td>['I found this place on TripAdvisor and wanted...</td>\n",
       "      <td>4.5 of 5 bubbles</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>['8 November 2023', '4 November 2023', '4 Nove...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4. Restaurant Buschbeck's</td>\n",
       "      <td>https://www.tripadvisor.in/Restaurant_Review-g...</td>\n",
       "      <td>German, European</td>\n",
       "      <td>[\"As my wife is a coeliac I had to research pl...</td>\n",
       "      <td>5.0 of 5 bubbles</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>['24 October 2023', '1 October 2023', '15 Augu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5. 100 Gramm Bar</td>\n",
       "      <td>https://www.tripadvisor.in/Restaurant_Review-g...</td>\n",
       "      <td>Bar, Eastern European</td>\n",
       "      <td>['This place is amaizing! I loved the vibe, th...</td>\n",
       "      <td>5.0 of 5 bubbles</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>['23 August 2023', '23 August 2023', '21 April...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5785</th>\n",
       "      <td>6356. Bao Club</td>\n",
       "      <td>https://www.tripadvisor.in/Restaurant_Review-g...</td>\n",
       "      <td>Chinese, Asian</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>211</td>\n",
       "      <td>6330</td>\n",
       "      <td>6356</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5786</th>\n",
       "      <td>6357. Wurstkessel im KaDeWe</td>\n",
       "      <td>https://www.tripadvisor.in/Restaurant_Review-g...</td>\n",
       "      <td>French, International</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>211</td>\n",
       "      <td>6330</td>\n",
       "      <td>6357</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5787</th>\n",
       "      <td>6358. World of Champagne (Champagne Bar)</td>\n",
       "      <td>https://www.tripadvisor.in/Restaurant_Review-g...</td>\n",
       "      <td>French, Bar</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>211</td>\n",
       "      <td>6330</td>\n",
       "      <td>6358</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5788</th>\n",
       "      <td>6359. Chibo</td>\n",
       "      <td>https://www.tripadvisor.in/Restaurant_Review-g...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>211</td>\n",
       "      <td>6330</td>\n",
       "      <td>6359</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5789</th>\n",
       "      <td>6360. Steckenpferd</td>\n",
       "      <td>https://www.tripadvisor.in/Restaurant_Review-g...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>211</td>\n",
       "      <td>6330</td>\n",
       "      <td>6360</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5790 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         title  \\\n",
       "0                                 1. Focaccino   \n",
       "1                      2. Cafe Couscous - Vege   \n",
       "2                                3. Hackethals   \n",
       "3                    4. Restaurant Buschbeck's   \n",
       "4                             5. 100 Gramm Bar   \n",
       "...                                        ...   \n",
       "5785                            6356. Bao Club   \n",
       "5786               6357. Wurstkessel im KaDeWe   \n",
       "5787  6358. World of Champagne (Champagne Bar)   \n",
       "5788                               6359. Chibo   \n",
       "5789                        6360. Steckenpferd   \n",
       "\n",
       "                                                    url  \\\n",
       "0     https://www.tripadvisor.in/Restaurant_Review-g...   \n",
       "1     https://www.tripadvisor.in/Restaurant_Review-g...   \n",
       "2     https://www.tripadvisor.in/Restaurant_Review-g...   \n",
       "3     https://www.tripadvisor.in/Restaurant_Review-g...   \n",
       "4     https://www.tripadvisor.in/Restaurant_Review-g...   \n",
       "...                                                 ...   \n",
       "5785  https://www.tripadvisor.in/Restaurant_Review-g...   \n",
       "5786  https://www.tripadvisor.in/Restaurant_Review-g...   \n",
       "5787  https://www.tripadvisor.in/Restaurant_Review-g...   \n",
       "5788  https://www.tripadvisor.in/Restaurant_Review-g...   \n",
       "5789  https://www.tripadvisor.in/Restaurant_Review-g...   \n",
       "\n",
       "                    cuisines  \\\n",
       "0          Italian, Sicilian   \n",
       "1     Healthy, Mediterranean   \n",
       "2                German, Bar   \n",
       "3           German, European   \n",
       "4      Bar, Eastern European   \n",
       "...                      ...   \n",
       "5785          Chinese, Asian   \n",
       "5786   French, International   \n",
       "5787             French, Bar   \n",
       "5788                     NaN   \n",
       "5789                     NaN   \n",
       "\n",
       "                                                reviews       star rating  \\\n",
       "0     [\"The atmosphere is very classy yet cozy. The ...  4.5 of 5 bubbles   \n",
       "1     [\"The best wraps in Berlin period. I suggest g...  5.0 of 5 bubbles   \n",
       "2     ['I found this place on TripAdvisor and wanted...  4.5 of 5 bubbles   \n",
       "3     [\"As my wife is a coeliac I had to research pl...  5.0 of 5 bubbles   \n",
       "4     ['This place is amaizing! I loved the vibe, th...  5.0 of 5 bubbles   \n",
       "...                                                 ...               ...   \n",
       "5785                                                NaN               NaN   \n",
       "5786                                                NaN               NaN   \n",
       "5787                                                NaN               NaN   \n",
       "5788                                                NaN               NaN   \n",
       "5789                                                NaN               NaN   \n",
       "\n",
       "      page number  data offset  restaurant  \\\n",
       "0               0            0           1   \n",
       "1               0            0           2   \n",
       "2               0            0           3   \n",
       "3               0            0           4   \n",
       "4               0            0           5   \n",
       "...           ...          ...         ...   \n",
       "5785          211         6330        6356   \n",
       "5786          211         6330        6357   \n",
       "5787          211         6330        6358   \n",
       "5788          211         6330        6359   \n",
       "5789          211         6330        6360   \n",
       "\n",
       "                                                  dates  \n",
       "0     ['13 November 2023', '23 September 2023', '28 ...  \n",
       "1     ['25 October 2023', '10 August 2023', '9 June ...  \n",
       "2     ['8 November 2023', '4 November 2023', '4 Nove...  \n",
       "3     ['24 October 2023', '1 October 2023', '15 Augu...  \n",
       "4     ['23 August 2023', '23 August 2023', '21 April...  \n",
       "...                                                 ...  \n",
       "5785                                                NaN  \n",
       "5786                                                NaN  \n",
       "5787                                                NaN  \n",
       "5788                                                NaN  \n",
       "5789                                                NaN  \n",
       "\n",
       "[5790 rows x 9 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "berlin_reviews_2    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3887145-8923-4088-b40c-18b3292ce4f7",
   "metadata": {},
   "source": [
    "### Is it useable tho?\n",
    "This Dataset has been updated 6 months ago! \n",
    "But seems like the reviews are still from late 2023 or older! \n",
    "Lets check if that's true!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b831bf28-3a27-4872-a10a-091066784219",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# convert each stringified list of date strings into actual datetime objects\n",
    "import ast\n",
    "\n",
    "# First tunring stringified lists to ACTUAL lists:\n",
    "berlin_reviews_2[\"dates\"] = berlin_reviews_2[\"dates\"].apply(\n",
    "    lambda x: ast.literal_eval(x) if isinstance(x, str) and x.startswith(\"[\") else x\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27146b34-8c90-431e-a7a6-641ee2466b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now getting lazy and using my preprocessing methods to turn lists of strings to list of dates\n",
    "from src.preprocessing import add_date_features\n",
    "berlin_reviews_2 = add_date_features(berlin_reviews_2, date_column=\"dates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b41a0f12-364c-4484-ba32-b992ee0c6436",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Timestamp('2023-11-13 00:00:00'),\n",
       " Timestamp('2023-09-23 00:00:00'),\n",
       " Timestamp('2023-08-28 00:00:00'),\n",
       " Timestamp('2023-08-11 00:00:00'),\n",
       " Timestamp('2023-08-08 00:00:00'),\n",
       " Timestamp('2023-08-01 00:00:00'),\n",
       " Timestamp('2023-07-01 00:00:00'),\n",
       " Timestamp('2023-06-10 00:00:00'),\n",
       " Timestamp('2023-05-14 00:00:00'),\n",
       " Timestamp('2023-05-10 00:00:00'),\n",
       " Timestamp('2023-04-11 00:00:00'),\n",
       " Timestamp('2023-01-31 00:00:00'),\n",
       " Timestamp('2023-01-13 00:00:00'),\n",
       " Timestamp('2022-12-28 00:00:00'),\n",
       " Timestamp('2022-12-04 00:00:00'),\n",
       " Timestamp('2022-06-22 00:00:00'),\n",
       " Timestamp('2022-05-16 00:00:00'),\n",
       " Timestamp('2022-05-11 00:00:00'),\n",
       " Timestamp('2022-02-10 00:00:00'),\n",
       " Timestamp('2022-02-09 00:00:00'),\n",
       " Timestamp('2021-08-11 00:00:00'),\n",
       " Timestamp('2020-09-03 00:00:00'),\n",
       " Timestamp('2020-07-24 00:00:00'),\n",
       " Timestamp('2020-01-03 00:00:00'),\n",
       " Timestamp('2019-12-26 00:00:00'),\n",
       " Timestamp('2019-10-09 00:00:00'),\n",
       " Timestamp('2019-10-01 00:00:00'),\n",
       " Timestamp('2019-09-28 00:00:00'),\n",
       " Timestamp('2019-06-14 00:00:00'),\n",
       " Timestamp('2019-05-03 00:00:00'),\n",
       " Timestamp('2019-04-15 00:00:00'),\n",
       " Timestamp('2019-04-15 00:00:00'),\n",
       " Timestamp('2019-03-28 00:00:00'),\n",
       " Timestamp('2019-03-18 00:00:00'),\n",
       " Timestamp('2019-03-13 00:00:00'),\n",
       " Timestamp('2018-12-05 00:00:00'),\n",
       " Timestamp('2018-11-22 00:00:00'),\n",
       " Timestamp('2018-11-21 00:00:00'),\n",
       " Timestamp('2018-10-27 00:00:00'),\n",
       " Timestamp('2018-10-26 00:00:00'),\n",
       " Timestamp('2018-10-23 00:00:00'),\n",
       " Timestamp('2018-10-23 00:00:00'),\n",
       " Timestamp('2018-10-23 00:00:00'),\n",
       " Timestamp('2018-10-05 00:00:00'),\n",
       " Timestamp('2018-09-24 00:00:00'),\n",
       " Timestamp('2018-09-05 00:00:00'),\n",
       " Timestamp('2018-09-04 00:00:00'),\n",
       " Timestamp('2018-09-04 00:00:00'),\n",
       " Timestamp('2018-08-21 00:00:00'),\n",
       " Timestamp('2018-08-13 00:00:00'),\n",
       " Timestamp('2018-07-04 00:00:00'),\n",
       " Timestamp('2018-06-12 00:00:00'),\n",
       " Timestamp('2018-06-04 00:00:00'),\n",
       " Timestamp('2018-05-19 00:00:00'),\n",
       " Timestamp('2018-05-19 00:00:00'),\n",
       " Timestamp('2018-05-18 00:00:00'),\n",
       " Timestamp('2018-05-09 00:00:00'),\n",
       " Timestamp('2018-05-03 00:00:00'),\n",
       " Timestamp('2018-05-02 00:00:00'),\n",
       " Timestamp('2018-05-02 00:00:00'),\n",
       " Timestamp('2018-05-01 00:00:00'),\n",
       " Timestamp('2018-05-01 00:00:00'),\n",
       " Timestamp('2018-04-30 00:00:00'),\n",
       " Timestamp('2018-03-15 00:00:00'),\n",
       " Timestamp('2018-03-11 00:00:00'),\n",
       " Timestamp('2018-03-03 00:00:00'),\n",
       " Timestamp('2018-01-22 00:00:00'),\n",
       " Timestamp('2018-01-22 00:00:00'),\n",
       " Timestamp('2018-01-15 00:00:00'),\n",
       " Timestamp('2017-12-19 00:00:00'),\n",
       " Timestamp('2017-12-14 00:00:00'),\n",
       " Timestamp('2017-12-06 00:00:00'),\n",
       " Timestamp('2017-11-19 00:00:00'),\n",
       " Timestamp('2017-11-17 00:00:00'),\n",
       " Timestamp('2017-11-12 00:00:00'),\n",
       " Timestamp('2017-10-22 00:00:00'),\n",
       " Timestamp('2017-10-21 00:00:00'),\n",
       " Timestamp('2017-10-06 00:00:00'),\n",
       " Timestamp('2017-09-27 00:00:00'),\n",
       " Timestamp('2017-08-28 00:00:00'),\n",
       " Timestamp('2017-08-23 00:00:00'),\n",
       " Timestamp('2017-08-13 00:00:00'),\n",
       " Timestamp('2017-06-18 00:00:00'),\n",
       " Timestamp('2017-06-07 00:00:00'),\n",
       " Timestamp('2017-05-18 00:00:00'),\n",
       " Timestamp('2017-05-08 00:00:00'),\n",
       " Timestamp('2017-04-23 00:00:00'),\n",
       " Timestamp('2017-04-23 00:00:00'),\n",
       " Timestamp('2017-04-15 00:00:00'),\n",
       " Timestamp('2017-04-14 00:00:00'),\n",
       " Timestamp('2017-04-12 00:00:00'),\n",
       " Timestamp('2017-04-11 00:00:00'),\n",
       " Timestamp('2017-04-09 00:00:00'),\n",
       " Timestamp('2017-04-03 00:00:00'),\n",
       " Timestamp('2017-03-29 00:00:00'),\n",
       " Timestamp('2017-03-23 00:00:00'),\n",
       " Timestamp('2017-03-08 00:00:00'),\n",
       " Timestamp('2017-02-21 00:00:00'),\n",
       " Timestamp('2017-02-19 00:00:00'),\n",
       " Timestamp('2017-02-15 00:00:00'),\n",
       " Timestamp('2017-01-28 00:00:00'),\n",
       " Timestamp('2017-01-25 00:00:00'),\n",
       " Timestamp('2017-01-22 00:00:00'),\n",
       " Timestamp('2017-01-19 00:00:00'),\n",
       " Timestamp('2017-01-09 00:00:00'),\n",
       " Timestamp('2016-12-20 00:00:00'),\n",
       " Timestamp('2016-12-13 00:00:00'),\n",
       " Timestamp('2016-12-11 00:00:00'),\n",
       " Timestamp('2016-12-05 00:00:00'),\n",
       " Timestamp('2016-12-01 00:00:00'),\n",
       " Timestamp('2016-11-22 00:00:00'),\n",
       " Timestamp('2016-11-18 00:00:00'),\n",
       " Timestamp('2016-11-16 00:00:00'),\n",
       " Timestamp('2016-11-12 00:00:00'),\n",
       " Timestamp('2016-11-02 00:00:00'),\n",
       " Timestamp('2016-11-01 00:00:00'),\n",
       " Timestamp('2016-10-29 00:00:00'),\n",
       " Timestamp('2016-10-27 00:00:00'),\n",
       " Timestamp('2016-10-23 00:00:00'),\n",
       " Timestamp('2016-10-22 00:00:00'),\n",
       " Timestamp('2016-10-20 00:00:00'),\n",
       " Timestamp('2016-10-07 00:00:00'),\n",
       " Timestamp('2016-09-29 00:00:00'),\n",
       " Timestamp('2016-09-28 00:00:00'),\n",
       " Timestamp('2016-09-22 00:00:00'),\n",
       " Timestamp('2016-09-13 00:00:00'),\n",
       " Timestamp('2016-08-11 00:00:00'),\n",
       " Timestamp('2016-07-29 00:00:00'),\n",
       " Timestamp('2016-07-28 00:00:00'),\n",
       " Timestamp('2016-07-25 00:00:00'),\n",
       " Timestamp('2016-07-25 00:00:00'),\n",
       " Timestamp('2016-07-22 00:00:00'),\n",
       " Timestamp('2016-07-21 00:00:00'),\n",
       " Timestamp('2016-07-21 00:00:00'),\n",
       " Timestamp('2016-07-20 00:00:00'),\n",
       " Timestamp('2016-07-20 00:00:00'),\n",
       " Timestamp('2016-07-18 00:00:00'),\n",
       " Timestamp('2016-07-15 00:00:00'),\n",
       " Timestamp('2016-07-14 00:00:00'),\n",
       " Timestamp('2016-07-13 00:00:00'),\n",
       " Timestamp('2016-07-11 00:00:00'),\n",
       " Timestamp('2016-07-02 00:00:00'),\n",
       " Timestamp('2016-06-28 00:00:00'),\n",
       " Timestamp('2016-06-26 00:00:00'),\n",
       " Timestamp('2016-06-21 00:00:00'),\n",
       " Timestamp('2016-06-15 00:00:00'),\n",
       " Timestamp('2016-05-29 00:00:00'),\n",
       " Timestamp('2016-05-29 00:00:00'),\n",
       " Timestamp('2016-05-25 00:00:00'),\n",
       " Timestamp('2016-05-24 00:00:00'),\n",
       " Timestamp('2016-05-23 00:00:00'),\n",
       " Timestamp('2016-05-22 00:00:00'),\n",
       " Timestamp('2016-05-21 00:00:00'),\n",
       " Timestamp('2016-05-18 00:00:00'),\n",
       " Timestamp('2016-05-18 00:00:00'),\n",
       " Timestamp('2016-05-17 00:00:00'),\n",
       " Timestamp('2016-04-29 00:00:00'),\n",
       " Timestamp('2016-04-27 00:00:00'),\n",
       " Timestamp('2016-04-26 00:00:00'),\n",
       " Timestamp('2016-03-30 00:00:00'),\n",
       " Timestamp('2016-03-27 00:00:00'),\n",
       " Timestamp('2016-03-06 00:00:00'),\n",
       " Timestamp('2016-03-05 00:00:00'),\n",
       " Timestamp('2016-02-17 00:00:00'),\n",
       " Timestamp('2016-02-09 00:00:00'),\n",
       " Timestamp('2015-11-07 00:00:00'),\n",
       " Timestamp('2015-10-30 00:00:00'),\n",
       " Timestamp('2015-10-29 00:00:00'),\n",
       " Timestamp('2015-10-19 00:00:00'),\n",
       " Timestamp('2015-10-13 00:00:00'),\n",
       " Timestamp('2015-10-13 00:00:00'),\n",
       " Timestamp('2015-10-10 00:00:00'),\n",
       " Timestamp('2015-10-08 00:00:00'),\n",
       " Timestamp('2015-09-30 00:00:00'),\n",
       " Timestamp('2015-09-29 00:00:00'),\n",
       " Timestamp('2015-09-22 00:00:00'),\n",
       " Timestamp('2015-09-20 00:00:00'),\n",
       " Timestamp('2015-09-16 00:00:00'),\n",
       " Timestamp('2015-09-13 00:00:00'),\n",
       " Timestamp('2015-09-12 00:00:00'),\n",
       " Timestamp('2015-09-11 00:00:00'),\n",
       " Timestamp('2015-09-06 00:00:00'),\n",
       " Timestamp('2015-08-31 00:00:00'),\n",
       " Timestamp('2015-08-24 00:00:00'),\n",
       " Timestamp('2015-08-22 00:00:00'),\n",
       " Timestamp('2015-08-20 00:00:00'),\n",
       " Timestamp('2015-08-19 00:00:00'),\n",
       " Timestamp('2015-08-15 00:00:00'),\n",
       " Timestamp('2015-08-13 00:00:00'),\n",
       " Timestamp('2015-07-19 00:00:00'),\n",
       " Timestamp('2015-07-14 00:00:00'),\n",
       " Timestamp('2015-07-04 00:00:00'),\n",
       " Timestamp('2015-06-15 00:00:00'),\n",
       " Timestamp('2015-06-10 00:00:00'),\n",
       " Timestamp('2015-05-25 00:00:00'),\n",
       " Timestamp('2015-04-24 00:00:00'),\n",
       " Timestamp('2015-03-27 00:00:00'),\n",
       " Timestamp('2015-03-21 00:00:00'),\n",
       " Timestamp('2015-03-05 00:00:00'),\n",
       " Timestamp('2015-02-28 00:00:00'),\n",
       " Timestamp('2015-02-25 00:00:00'),\n",
       " Timestamp('2015-02-22 00:00:00'),\n",
       " Timestamp('2015-02-11 00:00:00'),\n",
       " Timestamp('2015-01-30 00:00:00'),\n",
       " Timestamp('2014-12-05 00:00:00'),\n",
       " Timestamp('2014-09-29 00:00:00'),\n",
       " Timestamp('2014-09-26 00:00:00'),\n",
       " Timestamp('2014-09-10 00:00:00'),\n",
       " Timestamp('2014-06-23 00:00:00'),\n",
       " Timestamp('2013-12-12 00:00:00'),\n",
       " Timestamp('2013-10-11 00:00:00'),\n",
       " Timestamp('2013-07-30 00:00:00'),\n",
       " Timestamp('2012-07-12 00:00:00'),\n",
       " Timestamp('2012-05-28 00:00:00'),\n",
       " Timestamp('2012-02-13 00:00:00'),\n",
       " Timestamp('2012-01-14 00:00:00'),\n",
       " Timestamp('2011-08-19 00:00:00'),\n",
       " Timestamp('2023-11-13 00:00:00'),\n",
       " Timestamp('2023-09-23 00:00:00'),\n",
       " Timestamp('2023-08-28 00:00:00'),\n",
       " Timestamp('2023-08-11 00:00:00'),\n",
       " Timestamp('2023-08-08 00:00:00'),\n",
       " Timestamp('2023-08-01 00:00:00'),\n",
       " Timestamp('2023-07-01 00:00:00'),\n",
       " Timestamp('2023-06-10 00:00:00'),\n",
       " Timestamp('2023-05-14 00:00:00'),\n",
       " Timestamp('2023-05-10 00:00:00'),\n",
       " Timestamp('2023-04-11 00:00:00'),\n",
       " Timestamp('2023-01-31 00:00:00'),\n",
       " Timestamp('2023-01-13 00:00:00'),\n",
       " Timestamp('2022-12-28 00:00:00'),\n",
       " Timestamp('2022-12-04 00:00:00')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "berlin_reviews_2[\"dates_parsed\"].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ca35cd-1211-437a-9c1f-3733edf5e14c",
   "metadata": {},
   "source": [
    "Look above, weird behavior is noticed.. the first dates match the last dates... looks like some dates repeated... why?!\n",
    "Let's check some other rows!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cad1dded-dff7-4e5f-88cc-12b27352d5b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Timestamp('2023-10-25 00:00:00'),\n",
       " Timestamp('2023-08-10 00:00:00'),\n",
       " Timestamp('2023-06-09 00:00:00'),\n",
       " Timestamp('2023-04-18 00:00:00'),\n",
       " Timestamp('2023-03-12 00:00:00'),\n",
       " Timestamp('2023-02-10 00:00:00'),\n",
       " Timestamp('2023-02-07 00:00:00'),\n",
       " Timestamp('2023-01-21 00:00:00'),\n",
       " Timestamp('2023-01-06 00:00:00'),\n",
       " Timestamp('2022-12-10 00:00:00'),\n",
       " Timestamp('2022-09-14 00:00:00'),\n",
       " Timestamp('2022-06-16 00:00:00'),\n",
       " Timestamp('2022-06-03 00:00:00'),\n",
       " Timestamp('2022-03-04 00:00:00'),\n",
       " Timestamp('2021-06-24 00:00:00'),\n",
       " Timestamp('2021-04-17 00:00:00'),\n",
       " Timestamp('2020-07-31 00:00:00'),\n",
       " Timestamp('2019-12-21 00:00:00'),\n",
       " Timestamp('2019-08-21 00:00:00'),\n",
       " Timestamp('2019-08-20 00:00:00'),\n",
       " Timestamp('2019-08-14 00:00:00'),\n",
       " Timestamp('2019-07-04 00:00:00'),\n",
       " Timestamp('2019-06-14 00:00:00'),\n",
       " Timestamp('2019-06-13 00:00:00'),\n",
       " Timestamp('2019-06-07 00:00:00'),\n",
       " Timestamp('2019-06-04 00:00:00'),\n",
       " Timestamp('2019-06-01 00:00:00'),\n",
       " Timestamp('2019-05-24 00:00:00'),\n",
       " Timestamp('2019-05-11 00:00:00'),\n",
       " Timestamp('2019-01-31 00:00:00'),\n",
       " Timestamp('2019-01-08 00:00:00'),\n",
       " Timestamp('2018-12-05 00:00:00'),\n",
       " Timestamp('2018-11-29 00:00:00'),\n",
       " Timestamp('2018-11-16 00:00:00'),\n",
       " Timestamp('2018-11-09 00:00:00'),\n",
       " Timestamp('2018-11-09 00:00:00'),\n",
       " Timestamp('2018-10-24 00:00:00'),\n",
       " Timestamp('2018-10-24 00:00:00'),\n",
       " Timestamp('2018-10-18 00:00:00'),\n",
       " Timestamp('2018-10-15 00:00:00'),\n",
       " Timestamp('2018-08-25 00:00:00'),\n",
       " Timestamp('2018-07-25 00:00:00'),\n",
       " Timestamp('2018-04-07 00:00:00'),\n",
       " Timestamp('2017-07-15 00:00:00'),\n",
       " Timestamp('2016-09-05 00:00:00'),\n",
       " Timestamp('2016-05-29 00:00:00'),\n",
       " Timestamp('2016-03-06 00:00:00'),\n",
       " Timestamp('2023-10-25 00:00:00'),\n",
       " Timestamp('2023-08-10 00:00:00'),\n",
       " Timestamp('2023-06-09 00:00:00'),\n",
       " Timestamp('2023-04-18 00:00:00'),\n",
       " Timestamp('2023-03-12 00:00:00'),\n",
       " Timestamp('2023-02-10 00:00:00'),\n",
       " Timestamp('2023-02-07 00:00:00'),\n",
       " Timestamp('2023-01-21 00:00:00'),\n",
       " Timestamp('2023-01-06 00:00:00'),\n",
       " Timestamp('2022-12-10 00:00:00'),\n",
       " Timestamp('2022-09-14 00:00:00'),\n",
       " Timestamp('2022-06-16 00:00:00'),\n",
       " Timestamp('2022-06-03 00:00:00'),\n",
       " Timestamp('2022-03-04 00:00:00'),\n",
       " Timestamp('2021-06-24 00:00:00')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "berlin_reviews_2[\"dates_parsed\"].iloc[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32107f77-fc28-456b-bb01-a3d691934524",
   "metadata": {},
   "source": [
    "AGAIN!! We have EXACTLY 15 first dates matching the 15 last dates... that's a problem! Have i caused it? or the dataset has been like that since the beginning??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b26b528-168f-4047-adb6-e579e517646f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index nr = 3532\n",
      "[Timestamp('2018-08-08 00:00:00'), Timestamp('2015-09-13 00:00:00'), Timestamp('2015-08-04 00:00:00'), Timestamp('2015-06-25 00:00:00'), Timestamp('2015-04-20 00:00:00'), Timestamp('2012-09-23 00:00:00'), Timestamp('2018-08-08 00:00:00'), Timestamp('2015-09-13 00:00:00'), Timestamp('2015-08-04 00:00:00'), Timestamp('2015-06-25 00:00:00'), Timestamp('2015-04-20 00:00:00'), Timestamp('2012-09-23 00:00:00')]\n"
     ]
    }
   ],
   "source": [
    "# i check some random rows, to see if the exact 15 dates match, then i check the review! if the reviews are the same... then we need cleaning!!\n",
    "random_row = np.random.randint(0, 5000)\n",
    "print('index nr =', random_row)\n",
    "print(berlin_reviews_2[\"dates_parsed\"].iloc[random_row])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095c6e05-92a0-4a8f-b85e-fe869e6662f6",
   "metadata": {},
   "source": [
    "Some useful insights from the above algorythm:\n",
    "\n",
    "index nr = 4205\n",
    "\n",
    "[Timestamp('2023-09-27 00:00:00'), Timestamp('2023-09-27 00:00:00')]\n",
    "\n",
    "index nr = 2227\n",
    "\n",
    "['3 June 2023', '30 December 2019', '15 August 2019', '1 June 2019', '6 May 2019', '3 June 2023', '30 December 2019', '15 August 2019', '1 June 2019', '6 May 2019']\n",
    "\n",
    "index nr = 4675\n",
    "\n",
    "['28 May 2023', '28 May 2023']\n",
    "\n",
    "\n",
    "So now we have another challenge, when the nr of reviews and therefore the date of reviews are less than 15 in tottal!! \n",
    "### ACTION: Cleaning later needed!\n",
    "### Cleaning 1: Done [ ]  or  Due [ ]\n",
    "DO THIS: when less than 15 review dates, clean the last half! EZ PZ!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3cdd98f7-8e61-47ac-9c4b-d40dfba46a98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>earliest_date</th>\n",
       "      <th>latest_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011-08-19</td>\n",
       "      <td>2023-11-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-03-06</td>\n",
       "      <td>2023-10-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2008-01-19</td>\n",
       "      <td>2023-11-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-05-08</td>\n",
       "      <td>2023-10-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-05-07</td>\n",
       "      <td>2023-08-23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  earliest_date latest_date\n",
       "0    2011-08-19  2023-11-13\n",
       "1    2016-03-06  2023-10-25\n",
       "2    2008-01-19  2023-11-08\n",
       "3    2017-05-08  2023-10-24\n",
       "4    2019-05-07  2023-08-23"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now back to extracting earliest and latest review to see if the dataset is up to date as kaggle shows:\n",
    "berlin_reviews_2[[\"earliest_date\", \"latest_date\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "89d18179-4244-40a5-b154-f1a3b5f0f959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2007-11-01 00:00:00\n",
      "2023-11-20 00:00:00\n"
     ]
    }
   ],
   "source": [
    "print(berlin_reviews_2[\"earliest_date\"].min())\n",
    "print(berlin_reviews_2[\"latest_date\"].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c87d53-0f20-43b3-88e1-3cee4d1cfb59",
   "metadata": {},
   "source": [
    "### Summary of Dataset berlin_reviews_2:\n",
    "**1st Summary:**\n",
    "The author has updated this dataset 6 months ago on Kaggle, but still the last review goes back to 2023.\n",
    "In all honesty, just exploring random data on this data set, i could tell the latest reviews are mostly from 2023 and then 2022 and so on. so it def is not too old to maneuver with! But a more updated dataset is appreciated. Off the next dataset now!\n",
    "\n",
    "Newest Review:2023-11-20\n",
    "\n",
    "Oldest Review: 2007-11-01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757b56a7-ead4-43df-8b06-3e5ef59cd309",
   "metadata": {},
   "source": [
    "## 2. Foursquare API for berlin venues:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84381a8d-402a-4c2b-8069-4e44adaefabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @hidden_cell\n",
    "\n",
    "CLIENT_ID = 'HIE5ISYEZQEQPNSVGQGRWCJMXA43OC3MBRHICDU01GF1P0EA' # your Foursquare ID\n",
    "CLIENT_SECRET = 'DK3E0ME2RXUUXAOX54VSSULBVYBUJWUD4BRVAQIJMV2ZIG54' # your Foursquare Secret\n",
    "VERSION = '20190701' # Foursquare API version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "339e576e-1495-49ed-b2fd-55628570fe88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns a DataFrame with Venue details\n",
    "def getNearbyVenues(names, latitudes, longitudes, radius, LIMIT):\n",
    "    \n",
    "    venues_list=[]\n",
    "    for name, lat, lng in zip(names, latitudes, longitudes):\n",
    "        # create the API request URL\n",
    "        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(\n",
    "            CLIENT_ID, \n",
    "            CLIENT_SECRET, \n",
    "            VERSION, \n",
    "            lat, \n",
    "            lng, \n",
    "            radius, \n",
    "            LIMIT)\n",
    "            \n",
    "        # make the GET request\n",
    "        results = requests.get(url).json()[\"response\"]['groups'][0]['items']\n",
    "        # return only relevant information for each nearby venue\n",
    "        venues_list.append([(\n",
    "            name, \n",
    "            lat, \n",
    "            lng, \n",
    "            v['venue']['name'], \n",
    "            v['venue']['location']['lat'], \n",
    "            v['venue']['location']['lng'],  \n",
    "            v['venue']['categories'][0]['name']) for v in results])\n",
    "\n",
    "    nearby_venues = pd.DataFrame([item for venue_list in venues_list for item in venue_list])\n",
    "    nearby_venues.columns = ['Neighbourhood', \n",
    "                  'Neighbourhood Latitude', \n",
    "                  'Neighbourhood Longitude', \n",
    "                  'Venue', \n",
    "                  'Venue Latitude', \n",
    "                  'Venue Longitude', \n",
    "                  'Venue Category']\n",
    "    \n",
    "    return(nearby_venues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a7a4998-3c02-402c-ac0b-6735a3af6e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns a DataFrame with top venues based on a threshold\n",
    "def return_top_venues(row, maximum_venues):\n",
    "    \n",
    "    # Select all except neighbourhood column\n",
    "    row_categories = row.iloc[1:]\n",
    "    row_categories_sorted = row_categories.sort_values(ascending=False)\n",
    "    \n",
    "    return row_categories_sorted.index.values[0: maximum_venues]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27d5a820-cd4b-4cfa-bf41-4cb713c957de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching data, could be caused by exceeding Foursquare maximum calls/ day.\n",
      "Cannot view data: Berlin_venues DataFrame was not successfully created.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "# Try to fetch Berlin Venues data\n",
    "try:\n",
    "    Berlin_venues = getNearbyVenues(Berlin_neighbourhoods['Neighbourhood'],\n",
    "                                        Berlin_neighbourhoods['Latitude'],\n",
    "                                        Berlin_neighbourhoods['Longitude'],\n",
    "                                        radius=2000,\n",
    "                                        LIMIT=100)\n",
    "except:\n",
    "    print('Error fetching data, could be caused by exceeding Forsquare maximum calls/ day.')\n",
    "\n",
    "# View the DataFrame\n",
    "Berlin_venues.head()\n",
    "'''\n",
    "# 1. Initialize the variable outside the try block\n",
    "Berlin_venues = None\n",
    "\n",
    "# 2. Try to fetch Berlin Venues data\n",
    "try:\n",
    "    Berlin_venues = getNearbyVenues(Berlin_neighbourhoods['Neighbourhood'],\n",
    "                                    Berlin_neighbourhoods['Latitude'],\n",
    "                                    Berlin_neighbourhoods['Longitude'],\n",
    "                                    radius=2000,\n",
    "                                    LIMIT=100)\n",
    "except Exception as e: # Catch the specific exception for better debugging\n",
    "    print('Error fetching data, could be caused by exceeding Foursquare maximum calls/ day.')\n",
    "    # print(f\"Details: {e}\") # Uncomment to see the actual error details\n",
    "\n",
    "# 3. View the DataFrame ONLY IF it was successfully created\n",
    "if Berlin_venues is not None:\n",
    "    Berlin_venues.head()\n",
    "else:\n",
    "    print(\"Cannot view data: Berlin_venues DataFrame was not successfully created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c6e881b-f643-4425-a204-12d8dd7851f6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Berlin_neighbourhoods' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m Berlin_venues \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# 2. Slice the DataFrame to use only the first 5 neighborhoods\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m sample_neighbourhoods \u001b[38;5;241m=\u001b[39m \u001b[43mBerlin_neighbourhoods\u001b[49m\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâœ… Attempting to fetch venues for the first \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(sample_neighbourhoods)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m neighborhoods...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# 3. Try to fetch Berlin Venues data using the smaller sample\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Berlin_neighbourhoods' is not defined"
     ]
    }
   ],
   "source": [
    "# 1. Initialize the variable outside the try block\n",
    "Berlin_venues = None\n",
    "\n",
    "# 2. Slice the DataFrame to use only the first 5 neighborhoods\n",
    "sample_neighbourhoods = Berlin_neighbourhoods.head(5)\n",
    "\n",
    "print(f\"âœ… Attempting to fetch venues for the first {len(sample_neighbourhoods)} neighborhoods...\")\n",
    "\n",
    "# 3. Try to fetch Berlin Venues data using the smaller sample\n",
    "try:\n",
    "    Berlin_venues = getNearbyVenues(sample_neighbourhoods['Neighbourhood'],\n",
    "                                    sample_neighbourhoods['Latitude'],\n",
    "                                    sample_neighbourhoods['Longitude'],\n",
    "                                    radius=2000,\n",
    "                                    LIMIT=100)\n",
    "except Exception as e:\n",
    "    print('âŒ Error fetching data, could be caused by exceeding Foursquare maximum calls/ day or an issue with the API keys.')\n",
    "    # print(f\"Details: {e}\") # Uncomment to see the actual error details\n",
    "\n",
    "# 4. View the DataFrame ONLY IF it was successfully created\n",
    "if Berlin_venues is not None:\n",
    "    print(\"\\nðŸŽ‰ Success! Displaying the first 5 rows of the fetched data:\")\n",
    "    Berlin_venues.head()\n",
    "else:\n",
    "    print(\"\\nâš ï¸ Cannot view data: Berlin_venues DataFrame was not successfully created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e9b9a149-5c3e-4e81-9176-305652e5568a",
   "metadata": {},
   "outputs": [
    {
     "ename": "FeatureNotFound",
     "evalue": "Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFeatureNotFound\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Get HTML file of Berlin Neighbourhoods\u001b[39;00m\n\u001b[1;32m      2\u001b[0m Berlin_neighbourhoods_html \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://en.wikipedia.org/wiki/Boroughs_and_neighborhoods_of_Berlin\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mtext\n\u001b[0;32m----> 3\u001b[0m page \u001b[38;5;241m=\u001b[39m \u001b[43mBeautifulSoup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBerlin_neighbourhoods_html\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlxml\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Check Webpage Title\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1. Webpage Title: \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m----------------------\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(page\u001b[38;5;241m.\u001b[39mtitle\u001b[38;5;241m.\u001b[39mtext))\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/berlin_bites/lib/python3.10/site-packages/bs4/__init__.py:366\u001b[0m, in \u001b[0;36mBeautifulSoup.__init__\u001b[0;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[0m\n\u001b[1;32m    364\u001b[0m     possible_builder_class \u001b[38;5;241m=\u001b[39m builder_registry\u001b[38;5;241m.\u001b[39mlookup(\u001b[38;5;241m*\u001b[39mfeatures)\n\u001b[1;32m    365\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m possible_builder_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 366\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m FeatureNotFound(\n\u001b[1;32m    367\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find a tree builder with the features you \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    368\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequested: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m. Do you need to install a parser library?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    369\u001b[0m             \u001b[38;5;241m%\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(features)\n\u001b[1;32m    370\u001b[0m         )\n\u001b[1;32m    371\u001b[0m     builder_class \u001b[38;5;241m=\u001b[39m possible_builder_class\n\u001b[1;32m    373\u001b[0m \u001b[38;5;66;03m# At this point either we have a TreeBuilder instance in\u001b[39;00m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;66;03m# builder, or we have a builder_class that we can instantiate\u001b[39;00m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;66;03m# with the remaining **kwargs.\u001b[39;00m\n",
      "\u001b[0;31mFeatureNotFound\u001b[0m: Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?"
     ]
    }
   ],
   "source": [
    "# Get HTML file of Berlin Neighbourhoods\n",
    "Berlin_neighbourhoods_html = requests.get('https://en.wikipedia.org/wiki/Boroughs_and_neighborhoods_of_Berlin').text\n",
    "page = BeautifulSoup(Berlin_neighbourhoods_html, 'lxml')\n",
    "\n",
    "# Check Webpage Title\n",
    "print('1. Webpage Title: \\n----------------------\\n{} \\n \\n'.format(page.title.text))\n",
    "\n",
    "# Find the first table in the webpage\n",
    "table = page.find('table')\n",
    "\n",
    "# Create a DataFrame for Boroughs in Berlin\n",
    "Berlin_boroughs = pd.DataFrame()\n",
    "\n",
    "# Assign the retrived table to a list\n",
    "data_list = pd.read_html(str(table))\n",
    "\n",
    "# Copy retrived data to the Berlin_boroughs DataFrame\n",
    "Berlin_boroughs = data_list[0]\n",
    "\n",
    "# Rename Columns and drop the Map column\n",
    "Berlin_boroughs.drop(['Map'], axis =1, inplace=True)\n",
    "Berlin_boroughs.columns = ['Boroughs', 'Population', 'Area', 'Density']\n",
    "\n",
    "print('2. DataFrame Shape:\\n----------------------\\n', Berlin_boroughs.shape,'\\n \\n')\n",
    "Berlin_boroughs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686f05b3-a8cb-4046-9ccb-5bbda824ba6c",
   "metadata": {},
   "source": [
    "### **For NOW** i hold it with this FSQ data source! \n",
    "### It is more useful for geografical insights than reviews!\n",
    "But i have found some other veryy verryyy intersting data sources for super advanced data insights.\n",
    "\n",
    "These are as followed:\n",
    "\n",
    "https://kepler.gl/\n",
    "\n",
    "https://deck.gl/\n",
    "\n",
    "https://opensource.foursquare.com/os-places/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503c8327-25dd-4ab5-8e57-aa66c840967b",
   "metadata": {},
   "source": [
    "## 3. yelp: https://www.youtube.com/watch?v=mn6aj3JitVo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "79019ce9-6f8e-4188-81d4-4dabe1847a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_yelp_reviews(api_key, location='Berlin', term='restaurant', limit=50):\n",
    "    headers = {'Authorization': f'Bearer {api_key}'}\n",
    "    url = 'https://api.yelp.com/v3/businesses/search'\n",
    "    params = {'location': location, 'term': term, 'limit': limit}\n",
    "    response = requests.get(url, headers=headers, params=params).json()\n",
    "    \n",
    "    reviews_list = []\n",
    "    for business in response['businesses']:\n",
    "        reviews_list.append({\n",
    "            'name': business['name'],\n",
    "            'rating': business['rating'],\n",
    "            'review_count': business['review_count'],\n",
    "            'category': business['categories'][0]['title']\n",
    "        })\n",
    "    return pd.DataFrame(reviews_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a47bb3-a871-41d6-b669-d146c2e78960",
   "metadata": {},
   "source": [
    "ok so yelp only gives me 3 reviews per place... that's... not ideal for this project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8570aac6-787b-43e9-9097-cf180899298e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "from typing import List, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "13256c12-b89e-4656-8cfd-639f4a9b9dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "HEADERS = {\n",
    "    \"User-Agent\":\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "        \"Chrome/124.0.0.0 Safari/537.36 \"\n",
    "        \"Edg/124.0.2478.51\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "572a3f4f-2db1-45e9-8fb1-eee93769e433",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_yelp_berlin(term=\"restaurants\", pages=3) -> List[str]:\n",
    "    \"\"\"\n",
    "    Scrapes Yelp search results pages for restaurants in Berlin.\n",
    "    Returns list of restaurant page URLs.\n",
    "    \"\"\"\n",
    "    base_url = \"https://www.yelp.com/search\"\n",
    "    urls = []\n",
    "\n",
    "    for p in range(pages):\n",
    "        print(f\"[INFO] Fetching search page {p+1}/{pages}...\")\n",
    "        params = {\n",
    "            \"find_desc\": term,\n",
    "            \"find_loc\": \"Berlin, Germany\",\n",
    "            \"start\": p * 10\n",
    "        }\n",
    "\n",
    "        res = requests.get(base_url, headers=HEADERS, params=params)\n",
    "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "\n",
    "        # Yelp business links look like /biz/something\n",
    "        for link in soup.select(\"a.css-1k2tj2c\"):\n",
    "            href = link.get(\"href\", \"\")\n",
    "            if href.startswith(\"/biz/\") and \"?\" not in href:\n",
    "                full_url = \"https://www.yelp.com\" + href\n",
    "                urls.append(full_url)\n",
    "\n",
    "        time.sleep(random.uniform(1, 2))  # respectful scraping\n",
    "\n",
    "    # Remove duplicates\n",
    "    urls = list(dict.fromkeys(urls))\n",
    "    print(f\"[INFO] Found {len(urls)} restaurant URLs.\")\n",
    "    return urls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "668f3ff4-073e-4429-8ecb-d3b4d90ce630",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_review_block(block) -> Dict:\n",
    "    \"\"\"\n",
    "    Extract text, rating, date from a single Yelp review HTML block.\n",
    "    \"\"\"\n",
    "    # Review text\n",
    "    try:\n",
    "        text = block.select_one(\"span.raw__09f24__T4Ezm\").get_text(strip=True)\n",
    "    except:\n",
    "        text = \"\"\n",
    "\n",
    "    # Star rating\n",
    "    try:\n",
    "        rating_tag = block.select_one(\"div.i-stars__09f24__M1AR7\")\n",
    "        rating = float(rating_tag[\"aria-label\"].split()[0])\n",
    "    except:\n",
    "        rating = None\n",
    "\n",
    "    # Review date\n",
    "    try:\n",
    "        date = block.select_one(\"span.css-1e4fdj9\").get_text(strip=True)\n",
    "    except:\n",
    "        date = None\n",
    "\n",
    "    return {\n",
    "        \"review_text\": text,\n",
    "        \"review_rating\": rating,\n",
    "        \"review_date\": date\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1dcb14b2-e8d3-4d53-be8a-00d28f794083",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_restaurant_reviews(url: str, max_pages=20) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Scrape *all* review pages for one restaurant.\n",
    "    \"\"\"\n",
    "    print(f\"[SCRAPE] {url}\")\n",
    "\n",
    "    reviews = []\n",
    "\n",
    "    for p in range(max_pages):\n",
    "        paged_url = f\"{url}?start={p * 10}\"\n",
    "\n",
    "        res = requests.get(paged_url, headers=HEADERS)\n",
    "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "\n",
    "        # review containers\n",
    "        review_blocks = soup.select(\"div.review__09f24__oHr9V\")\n",
    "\n",
    "        if not review_blocks:\n",
    "            # no more review pages\n",
    "            break\n",
    "\n",
    "        for block in review_blocks:\n",
    "            rev = parse_review_block(block)\n",
    "            rev[\"restaurant_url\"] = url\n",
    "            reviews.append(rev)\n",
    "\n",
    "        time.sleep(random.uniform(1.0, 2.0))\n",
    "\n",
    "    print(f\"[DONE] {len(reviews)} reviews scraped.\")\n",
    "    return reviews\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "32ebda25-e185-4621-a0a6-cdd10f85f428",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_berlin_reviews(term=\"restaurants\", pages=3, max_review_pages=20):\n",
    "    \"\"\"\n",
    "    Scrape Yelp for Berlin restaurants and collect all their reviews.\n",
    "    Saves a CSV in data/raw.\n",
    "    \"\"\"\n",
    "    urls = search_yelp_berlin(term=term, pages=pages)\n",
    "    all_reviews = []\n",
    "\n",
    "    for url in urls:\n",
    "        try:\n",
    "            restaurant_reviews = scrape_restaurant_reviews(url, max_pages=max_review_pages)\n",
    "            all_reviews.extend(restaurant_reviews)\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Failed scraping {url}: {e}\")\n",
    "            continue\n",
    "\n",
    "    df = pd.DataFrame(all_reviews)\n",
    "    df[\"crawled_at\"] = datetime.utcnow().isoformat()\n",
    "\n",
    "    # Save raw dataset\n",
    "    df.to_csv(\"../data/raw/yelp_berlin_reviews_raw.csv\", index=False)\n",
    "    print(\"[SAVE] Saved to ../data/raw/yelp_berlin_reviews_raw.csv\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "503ae5da-c39b-4c1e-87f8-63e763d08520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Fetching search page 1/3...\n",
      "[INFO] Fetching search page 2/3...\n",
      "[INFO] Fetching search page 3/3...\n",
      "[INFO] Found 0 restaurant URLs.\n",
      "[SAVE] Saved to ../data/raw/yelp_berlin_reviews_raw.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>crawled_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [crawled_at]\n",
       "Index: []"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = collect_berlin_reviews(\n",
    "    term=\"restaurants\", \n",
    "    pages=3,            # 3 search pages â†’ ~30 restaurants\n",
    "    max_review_pages=15 # scrape up to 150 reviews per restaurant\n",
    ")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "389f8aa8-15cb-4bad-b0e4-7580f0dd626a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status code: 403\n",
      "HTML length: 0\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://www.yelp.com/search?find_desc=restaurants&find_loc=Berlin%2C+Germany\"\n",
    "res = requests.get(url, headers=HEADERS)\n",
    "print(\"Status code:\", res.status_code)\n",
    "print(\"HTML length:\", len(res.text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f7ab2bcf-75ea-41ad-b374-bce497582b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "US_PROXIES = [\n",
    "    \"http://104.248.63.17:30588\",\n",
    "    \"http://67.213.212.47:52215\",\n",
    "    \"http://38.153.137.2:999\",\n",
    "    \"http://154.236.168.179:1976\",\n",
    "    \"http://44.226.167.102:3128\",\n",
    "    \"http://50.18.33.231:8080\",\n",
    "    \"http://20.110.168.104:80\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "36d3f05d-01d8-4622-bbc4-1c92458d4996",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_proxy():\n",
    "    proxy = random.choice(US_PROXIES)\n",
    "    return {\n",
    "        \"http\": proxy,\n",
    "        \"https\": proxy\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d0ae5eaa-b800-4131-bebf-46cde918127d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [403]>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requests.get(url, headers=HEADERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9b4ca86d-28ca-4af3-bbcc-2f32f0470c72",
   "metadata": {},
   "outputs": [
    {
     "ename": "ReadTimeout",
     "evalue": "HTTPSConnectionPool(host='www.yelp.com', port=443): Read timed out. (read timeout=10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/berlin_bites/lib/python3.10/site-packages/urllib3/connectionpool.py:773\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 773\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_proxy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    774\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m, SocketTimeout) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/berlin_bites/lib/python3.10/site-packages/urllib3/connectionpool.py:1042\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._prepare_proxy\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1036\u001b[0m conn\u001b[38;5;241m.\u001b[39mset_tunnel(\n\u001b[1;32m   1037\u001b[0m     scheme\u001b[38;5;241m=\u001b[39mtunnel_scheme,\n\u001b[1;32m   1038\u001b[0m     host\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tunnel_host,\n\u001b[1;32m   1039\u001b[0m     port\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mport,\n\u001b[1;32m   1040\u001b[0m     headers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproxy_headers,\n\u001b[1;32m   1041\u001b[0m )\n\u001b[0;32m-> 1042\u001b[0m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/berlin_bites/lib/python3.10/site-packages/urllib3/connection.py:770\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_connected_to_proxy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 770\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tunnel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;66;03m# Override the host with the one we're requesting data from.\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/berlin_bites/lib/python3.10/site-packages/urllib3/connection.py:265\u001b[0m, in \u001b[0;36mHTTPConnection._tunnel\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 265\u001b[0m     (version, code, message) \u001b[38;5;241m=\u001b[39m \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    267\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m code \u001b[38;5;241m!=\u001b[39m http\u001b[38;5;241m.\u001b[39mHTTPStatus\u001b[38;5;241m.\u001b[39mOK:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/lib/python3.10/http/client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 279\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n",
      "\u001b[0;31mTimeoutError\u001b[0m: timed out",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mReadTimeoutError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/berlin_bites/lib/python3.10/site-packages/requests/adapters.py:644\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    643\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 644\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    645\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    646\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    647\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    648\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    651\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    652\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    653\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    654\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    655\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/berlin_bites/lib/python3.10/site-packages/urllib3/connectionpool.py:841\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    839\u001b[0m     new_e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, new_e)\n\u001b[0;32m--> 841\u001b[0m retries \u001b[38;5;241m=\u001b[39m \u001b[43mretries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    842\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    844\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/berlin_bites/lib/python3.10/site-packages/urllib3/util/retry.py:474\u001b[0m, in \u001b[0;36mRetry.increment\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m read \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_method_retryable(method):\n\u001b[0;32m--> 474\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    475\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m read \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/berlin_bites/lib/python3.10/site-packages/urllib3/util/util.py:39\u001b[0m, in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m value\u001b[38;5;241m.\u001b[39mwith_traceback(tb)\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m value\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/berlin_bites/lib/python3.10/site-packages/urllib3/connectionpool.py:775\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    774\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m, SocketTimeout) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 775\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_timeout\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    776\u001b[0m \u001b[43m        \u001b[49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproxy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    777\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    778\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/berlin_bites/lib/python3.10/site-packages/urllib3/connectionpool.py:367\u001b[0m, in \u001b[0;36mHTTPConnectionPool._raise_timeout\u001b[0;34m(self, err, url, timeout_value)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(err, SocketTimeout):\n\u001b[0;32m--> 367\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ReadTimeoutError(\n\u001b[1;32m    368\u001b[0m         \u001b[38;5;28mself\u001b[39m, url, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRead timed out. (read timeout=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimeout_value\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    369\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;66;03m# See the above comment about EAGAIN in Python 3.\u001b[39;00m\n",
      "\u001b[0;31mReadTimeoutError\u001b[0m: HTTPSConnectionPool(host='www.yelp.com', port=443): Read timed out. (read timeout=10)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mReadTimeout\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mHEADERS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mget_random_proxy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/berlin_bites/lib/python3.10/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/berlin_bites/lib/python3.10/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/berlin_bites/lib/python3.10/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/berlin_bites/lib/python3.10/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/berlin_bites/lib/python3.10/site-packages/requests/adapters.py:690\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    688\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m    689\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, ReadTimeoutError):\n\u001b[0;32m--> 690\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ReadTimeout(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m    691\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, _InvalidHeader):\n\u001b[1;32m    692\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidHeader(e, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "\u001b[0;31mReadTimeout\u001b[0m: HTTPSConnectionPool(host='www.yelp.com', port=443): Read timed out. (read timeout=10)"
     ]
    }
   ],
   "source": [
    "requests.get(url, headers=HEADERS, proxies=get_random_proxy(), timeout=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd3b95db-d3ef-418d-98ab-158e91456a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_yelp_berlin(term=\"restaurants\", pages=3):\n",
    "    \"\"\"\n",
    "    Scrapes Yelp search results for Berlin.\n",
    "    Returns a list of restaurant URLs.\n",
    "    \"\"\"\n",
    "    base_url = \"https://www.yelp.com/search\"\n",
    "    urls = []\n",
    "\n",
    "    for p in range(pages):\n",
    "        print(f\"[INFO] Fetching search page {p+1}/{pages}...\")\n",
    "\n",
    "        params = {\n",
    "            \"find_desc\": term,\n",
    "            \"find_loc\": \"Berlin\",\n",
    "            \"start\": p * 10\n",
    "        }\n",
    "\n",
    "        # âœ… request the search page (works if VPN is on)\n",
    "        res = requests.get(base_url, headers=HEADERS, params=params)\n",
    "\n",
    "        if res.status_code != 200:\n",
    "            print(f\"[WARN] Status {res.status_code}\")\n",
    "            continue\n",
    "\n",
    "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "\n",
    "        # âœ… extract all business URLs containing /biz/\n",
    "        for a in soup.find_all(\"a\", href=True):\n",
    "            href = a[\"href\"]\n",
    "            if href.startswith(\"/biz/\"):\n",
    "                full_url = \"https://www.yelp.com\" + href\n",
    "                urls.append(full_url)\n",
    "\n",
    "        time.sleep(random.uniform(1.2, 2.0))\n",
    "\n",
    "    # remove duplicates\n",
    "    urls = list(dict.fromkeys(urls))\n",
    "    print(f\"[INFO] Total restaurants found: {len(urls)}\")\n",
    "    return urls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "719e56d7-f580-46ef-9958-9f6443b2b16c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Fetching search page 1/1...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'HEADERS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#base_url = \"https://www.yelp.com/search?find_desc=restaurants&find_loc=Berlin\"\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m urls \u001b[38;5;241m=\u001b[39m \u001b[43msearch_yelp_berlin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrestaurants\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m urls[:\u001b[38;5;241m10\u001b[39m]\n",
      "Cell \u001b[0;32mIn[2], line 19\u001b[0m, in \u001b[0;36msearch_yelp_berlin\u001b[0;34m(term, pages)\u001b[0m\n\u001b[1;32m     12\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfind_desc\u001b[39m\u001b[38;5;124m\"\u001b[39m: term,\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfind_loc\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBerlin\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstart\u001b[39m\u001b[38;5;124m\"\u001b[39m: p \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m     16\u001b[0m }\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# âœ… request the search page (works if VPN is on)\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m res \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(base_url, headers\u001b[38;5;241m=\u001b[39m\u001b[43mHEADERS\u001b[49m, params\u001b[38;5;241m=\u001b[39mparams)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m res\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[WARN] Status \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mres\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'HEADERS' is not defined"
     ]
    }
   ],
   "source": [
    "#base_url = \"https://www.yelp.com/search?find_desc=restaurants&find_loc=Berlin\"\n",
    "urls = search_yelp_berlin(\"restaurants\", pages=1)\n",
    "urls[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d37100a1-f8b3-493c-9296-af81c5ba238f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from typing import List, Dict, Optional\n",
    "from urllib.parse import urljoin, urlencode, quote_plus\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(\"data_collection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "52be621e-fc9c-4e82-bb9b-bf07d566e245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Configuration / Globals\n",
    "# -----------------------\n",
    "BASE_SEARCH_URL = \"https://www.yelp.com/search\"\n",
    "YELP_BASE = \"https://www.yelp.com\"\n",
    "DEFAULT_LOCATION = \"Berlin, Germany\"\n",
    "\n",
    "# Example user-agents list to rotate\n",
    "USER_AGENTS = [\n",
    "    # common desktop agents (rotate for stealth)\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko)\"\n",
    "    \" Chrome/117.0.0.0 Safari/537.36 Edg/117.0.2045.60\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15\"\n",
    "    \" (KHTML, like Gecko) Version/14.0.3 Safari/605.1.15\",\n",
    "    \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko)\"\n",
    "    \" Chrome/115.0.0.0 Safari/537.36\",\n",
    "]\n",
    "\n",
    "DEFAULT_HEADERS = {\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8\",\n",
    "    # User-Agent will be added dynamically\n",
    "}\n",
    "\n",
    "# ScraperAPI config (if you have an API key, set env var SCRAPERAPI_KEY)\n",
    "SCRAPERAPI_KEY = os.getenv(\"SCRAPERAPI_KEY\", None)\n",
    "\n",
    "# -----------------------\n",
    "# Utilities\n",
    "# -----------------------\n",
    "def pick_headers():\n",
    "    h = DEFAULT_HEADERS.copy()\n",
    "    h[\"User-Agent\"] = random.choice(USER_AGENTS)\n",
    "    return h\n",
    "\n",
    "def safe_get(url, params=None, headers=None, timeout=10):\n",
    "    \"\"\"\n",
    "    Lightweight requests GET with retries and sensible timeouts.\n",
    "    \"\"\"\n",
    "    headers = headers or pick_headers()\n",
    "    params = params or {}\n",
    "    try_count = 0\n",
    "    while try_count < 3:\n",
    "        try:\n",
    "            resp = requests.get(url, params=params, headers=headers, timeout=timeout)\n",
    "            if resp.status_code == 200:\n",
    "                return resp\n",
    "            else:\n",
    "                logger.warning(\"GET %s returned status %s\", url, resp.status_code)\n",
    "                # cooldown then retry\n",
    "                time.sleep(1 + try_count)\n",
    "        except requests.RequestException as e:\n",
    "            logger.warning(\"Request exception: %s\", e)\n",
    "            time.sleep(1 + try_count)\n",
    "        try_count += 1\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "286b34fc-a7f7-49bf-9b7b-3fb532174d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Strategy 1: stealth requests\n",
    "# -----------------------\n",
    "def scrape_search_requests(query=\"restaurants\", location=DEFAULT_LOCATION, pages=2, per_page=10):\n",
    "    \"\"\"\n",
    "    Scrape Yelp search result pages for business links using requests + BeautifulSoup.\n",
    "    pages: number of search pages to attempt (Yelp uses `start` param offset 10 per page)\n",
    "    returns list of business page urls (absolute)\n",
    "    \"\"\"\n",
    "    found = []\n",
    "    for p in range(pages):\n",
    "        start = p * per_page  # Yelp uses start=0,10,20...\n",
    "        params = {\n",
    "            \"find_desc\": query,\n",
    "            \"find_loc\": location,\n",
    "            \"start\": start\n",
    "        }\n",
    "        logger.info(\"Requests search page %d (start=%s)\", p+1, start)\n",
    "        resp = safe_get(BASE_SEARCH_URL, params=params)\n",
    "        if not resp:\n",
    "            logger.warning(\"No response for search page %d\", p+1)\n",
    "            continue\n",
    "\n",
    "        soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "        # find links that look like /biz/<slug> (business pages)\n",
    "        anchors = soup.find_all(\"a\", href=True)\n",
    "        for a in anchors:\n",
    "            href = a[\"href\"]\n",
    "            # some links are to photos or other pages. business links commonly start with '/biz/'\n",
    "            if href.startswith(\"/biz/\"):\n",
    "                # normalize\n",
    "                full = urljoin(YELP_BASE, href.split(\"?\")[0])\n",
    "                if full not in found:\n",
    "                    found.append(full)\n",
    "        # polite pause\n",
    "        time.sleep(random.uniform(1.0, 2.0))\n",
    "    logger.info(\"Requests search found %d business links\", len(found))\n",
    "    return found\n",
    "\n",
    "def fetch_reviews_requests(biz_url, max_pages=2):\n",
    "    \"\"\"\n",
    "    Fetch reviews from a business page using requests and BeautifulSoup.\n",
    "    Returns list of reviews dicts with { 'biz_url', 'review_text', 'rating', 'date' }\n",
    "    NOTE: Yelp HTML structure is unstable; this function uses heuristics and may need tuning.\n",
    "    \"\"\"\n",
    "    reviews = []\n",
    "    resp = safe_get(biz_url)\n",
    "    if not resp:\n",
    "        return reviews\n",
    "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "    # Yelp uses a few classes for review text; fallback to <p> tags inside review containers\n",
    "    # Strategy: find review containers (divs) that include 'review' in class or role attributes.\n",
    "    containers = soup.find_all(lambda tag: tag.name == \"div\" and tag.get(\"data-testid\") == \"reviews-carousel\")  # sometimes\n",
    "    if not containers:\n",
    "        # fallback: find all divs that contain a 'span' that looks like rating and a 'p'\n",
    "        possible = soup.find_all(\"div\")\n",
    "        containers = []\n",
    "        for div in possible:\n",
    "            if div.find(\"p\") and div.find(\"span\"):\n",
    "                containers.append(div)\n",
    "\n",
    "    # Bruteforce parse: find <p> tags that are long enough to be reviews\n",
    "    p_tags = soup.find_all(\"p\")\n",
    "    for p in p_tags:\n",
    "        txt = p.get_text(strip=True)\n",
    "        if len(txt) > 50:  # heuristic: review longer than 50 chars\n",
    "            # try to find an associated date and rating nearby\n",
    "            parent = p.parent\n",
    "            date = None\n",
    "            rating = None\n",
    "            # look for time or span elements with dates\n",
    "            time_tag = parent.find(\"time\")\n",
    "            if time_tag and time_tag.has_attr(\"datetime\"):\n",
    "                date = time_tag[\"datetime\"]\n",
    "            else:\n",
    "                # find nearest text that looks like \"13 November 2023\"\n",
    "                # naive search in parent text\n",
    "                parent_text = parent.get_text(\" \", strip=True)\n",
    "                # not robust; just leave as None if not found\n",
    "            # rating: find aria-label like \"5.0 star rating\"\n",
    "            aria = parent.find(lambda tag: tag.has_attr(\"aria-label\") and \"star\" in tag[\"aria-label\"].lower()) \n",
    "            if aria:\n",
    "                try:\n",
    "                    rating = float(aria[\"aria-label\"].split()[0])\n",
    "                except Exception:\n",
    "                    rating = None\n",
    "            reviews.append({\"biz_url\": biz_url, \"review_text\": txt, \"date\": date, \"rating\": rating})\n",
    "    logger.info(\"Parsed %d candidate reviews from %s\", len(reviews), biz_url)\n",
    "    return reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2fbd3674-1ab6-42e3-8f51-ba843fd78521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Strategy 2: Selenium (Edge) â€” mimic human\n",
    "# -----------------------\n",
    "def scrape_with_selenium_edge(query=\"restaurants\", location=DEFAULT_LOCATION, pages=2, edge_driver_path=None, headless=True):\n",
    "    \"\"\"\n",
    "    Use Selenium + Edge to open the search pages, scroll to load content, and extract business links.\n",
    "    Requires: selenium, webdriver-manager (optional) and Microsoft Edge installed.\n",
    "    If edge_driver_path is None, will attempt to use webdriver_manager to fetch the driver.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from selenium import webdriver\n",
    "        from selenium.webdriver.edge.service import Service as EdgeService\n",
    "        from selenium.webdriver.common.by import By\n",
    "        from selenium.webdriver.common.keys import Keys\n",
    "        from selenium.webdriver.edge.options import Options as EdgeOptions\n",
    "        # optional driver manager\n",
    "        try:\n",
    "            from webdriver_manager.microsoft import EdgeChromiumDriverManager\n",
    "            manager_available = True\n",
    "        except Exception:\n",
    "            manager_available = False\n",
    "    except Exception as e:\n",
    "        logger.error(\"Selenium or webdriver-manager not installed: %s\", e)\n",
    "        return []\n",
    "\n",
    "    options = EdgeOptions()\n",
    "    if headless:\n",
    "        options.add_argument(\"--headless=new\")\n",
    "    options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    options.add_argument(\"--disable-extensions\")\n",
    "    options.add_argument(\"--start-maximized\")\n",
    "    options.add_argument(f\"user-agent={random.choice(USER_AGENTS)}\")\n",
    "\n",
    "    # get service\n",
    "    if edge_driver_path:\n",
    "        service = EdgeService(edge_driver_path)\n",
    "    else:\n",
    "        if manager_available:\n",
    "            drv_path = EdgeChromiumDriverManager().install()\n",
    "            service = EdgeService(drv_path)\n",
    "        else:\n",
    "            # rely on system PATH msedgedriver\n",
    "            service = EdgeService()\n",
    "\n",
    "    driver = webdriver.Edge(service=service, options=options)\n",
    "\n",
    "    found = []\n",
    "    try:\n",
    "        for p in range(pages):\n",
    "            start = p * 10\n",
    "            params = {\"find_desc\": query, \"find_loc\": location, \"start\": start}\n",
    "            url = BASE_SEARCH_URL + \"?\" + urlencode(params)\n",
    "            logger.info(\"Selenium loading %s\", url)\n",
    "            driver.get(url)\n",
    "            time.sleep(2 + random.random() * 2)\n",
    "\n",
    "            # scroll to bottom a few times to load lazy content\n",
    "            for _ in range(3):\n",
    "                driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                time.sleep(1 + random.random())\n",
    "\n",
    "            # find links to business pages; Yelp business links contain '/biz/'\n",
    "            elems = driver.find_elements(By.XPATH, \"//a[contains(@href, '/biz/')]\")\n",
    "            for e in elems:\n",
    "                href = e.get_attribute(\"href\")\n",
    "                if href and \"/biz/\" in href:\n",
    "                    normalized = href.split(\"?\")[0]\n",
    "                    if normalized not in found:\n",
    "                        found.append(normalized)\n",
    "            time.sleep(1 + random.random())\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "    logger.info(\"Selenium found %d business links\", len(found))\n",
    "    return found\n",
    "\n",
    "def fetch_reviews_selenium(biz_url, edge_driver_path=None, headless=True):\n",
    "    \"\"\"\n",
    "    Open business page and extract reviews with Selenium.\n",
    "    Returns list of dicts: {biz_url, review_text, rating, date}\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from selenium import webdriver\n",
    "        from selenium.webdriver.edge.service import Service as EdgeService\n",
    "        from selenium.webdriver.common.by import By\n",
    "        from selenium.webdriver.edge.options import Options as EdgeOptions\n",
    "        try:\n",
    "            from webdriver_manager.microsoft import EdgeChromiumDriverManager\n",
    "            manager_available = True\n",
    "        except Exception:\n",
    "            manager_available = False\n",
    "    except Exception as e:\n",
    "        logger.error(\"Selenium not available: %s\", e)\n",
    "        return []\n",
    "\n",
    "    options = EdgeOptions()\n",
    "    if headless:\n",
    "        options.add_argument(\"--headless=new\")\n",
    "    options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    options.add_argument(f\"user-agent={random.choice(USER_AGENTS)}\")\n",
    "\n",
    "    if edge_driver_path:\n",
    "        service = EdgeService(edge_driver_path)\n",
    "    else:\n",
    "        if manager_available:\n",
    "            drv_path = EdgeChromiumDriverManager().install()\n",
    "            service = EdgeService(drv_path)\n",
    "        else:\n",
    "            service = EdgeService()\n",
    "\n",
    "    driver = webdriver.Edge(service=service, options=options)\n",
    "    results = []\n",
    "    try:\n",
    "        logger.info(\"Selenium opening business %s\", biz_url)\n",
    "        driver.get(biz_url)\n",
    "        time.sleep(2 + random.random() * 2)\n",
    "\n",
    "        # scroll a bit to load reviews\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight/4);\")\n",
    "        time.sleep(1.5)\n",
    "\n",
    "        # typical Yelp reviews are in <p> tags; try to extract review paragraphs and rating elements\n",
    "        review_ps = driver.find_elements(By.XPATH, \"//p\")\n",
    "        for p in review_ps:\n",
    "            text = p.text\n",
    "            if text and len(text) > 50:\n",
    "                # try find rating in ancestor nodes\n",
    "                try:\n",
    "                    parent = p.find_element(By.XPATH, \"./ancestor::div[1]\")\n",
    "                    rating_elem = parent.find_element(By.XPATH, \".//div[contains(@aria-label,'star rating') or contains(@aria-label,'star rating')]/@aria-label\")\n",
    "                    rating = None\n",
    "                except Exception:\n",
    "                    rating = None\n",
    "                # date search via time tag\n",
    "                date = None\n",
    "                try:\n",
    "                    t = parent.find_element(By.XPATH, \".//time\")\n",
    "                    date = t.get_attribute(\"datetime\")\n",
    "                except Exception:\n",
    "                    date = None\n",
    "                results.append({\"biz_url\": biz_url, \"review_text\": text, \"date\": date, \"rating\": rating})\n",
    "    except Exception as e:\n",
    "        logger.exception(\"Error fetching reviews with selenium: %s\", e)\n",
    "    finally:\n",
    "        driver.quit()\n",
    "    logger.info(\"Selenium parsed %d reviews\", len(results))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d0958238-2a69-406b-b3f6-9c03a43c8a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Strategy 3: ScraperAPI\n",
    "# -----------------------\n",
    "def fetch_via_scraperapi(target_url, api_key=None):\n",
    "    \"\"\"\n",
    "    Use ScraperAPI to fetch the rendered HTML (ScraperAPI handles blocking).\n",
    "    Set SCRAPERAPI_KEY env var or pass api_key.\n",
    "    \"\"\"\n",
    "    key = api_key or SCRAPERAPI_KEY\n",
    "    if not key:\n",
    "        raise RuntimeError(\"No ScraperAPI key provided. Set SCRAPERAPI_KEY environment variable or pass api_key.\")\n",
    "\n",
    "    payload = {\n",
    "        \"api_key\": key,\n",
    "        \"url\": target_url,\n",
    "        \"country_code\": \"us\",\n",
    "        \"render\": \"true\",  # let ScraperAPI render JS\n",
    "        \"keep_headers\": \"false\"\n",
    "    }\n",
    "    url = \"http://api.scraperapi.com/?\" + urlencode(payload)\n",
    "    resp = safe_get(url, headers=pick_headers(), timeout=20)\n",
    "    if resp and resp.status_code == 200:\n",
    "        return resp.text\n",
    "    return None\n",
    "\n",
    "def scrape_search_scraperapi(query=\"restaurants\", location=DEFAULT_LOCATION, pages=2, api_key=None):\n",
    "    found = []\n",
    "    for p in range(pages):\n",
    "        start = p * 10\n",
    "        params = {\"find_desc\": query, \"find_loc\": location, \"start\": start}\n",
    "        target = BASE_SEARCH_URL + \"?\" + urlencode(params)\n",
    "        logger.info(\"ScraperAPI requesting %s\", target)\n",
    "        html = fetch_via_scraperapi(target, api_key=api_key)\n",
    "        if not html:\n",
    "            continue\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        for a in soup.find_all(\"a\", href=True):\n",
    "            if a[\"href\"].startswith(\"/biz/\"):\n",
    "                full = urljoin(YELP_BASE, a[\"href\"].split(\"?\")[0])\n",
    "                if full not in found:\n",
    "                    found.append(full)\n",
    "        time.sleep(1)\n",
    "    logger.info(\"ScraperAPI found %d business links\", len(found))\n",
    "    return found\n",
    "\n",
    "def fetch_reviews_scraperapi(biz_url, api_key=None):\n",
    "    html = fetch_via_scraperapi(biz_url, api_key=api_key)\n",
    "    if not html:\n",
    "        return []\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    reviews = []\n",
    "    for p in soup.find_all(\"p\"):\n",
    "        txt = p.get_text(strip=True)\n",
    "        if len(txt) > 50:\n",
    "            # try to find date/time element\n",
    "            parent = p.parent\n",
    "            date = None\n",
    "            t = parent.find(\"time\")\n",
    "            if t and t.has_attr(\"datetime\"):\n",
    "                date = t[\"datetime\"]\n",
    "            reviews.append({\"biz_url\": biz_url, \"review_text\": txt, \"date\": date, \"rating\": None})\n",
    "    logger.info(\"ScraperAPI parsed %d reviews for %s\", len(reviews), biz_url)\n",
    "    return reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "422a967d-6701-4c7b-a357-4126e284197b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlencode, urljoin\n",
    "from bs4 import BeautifulSoup\n",
    "import time, logging\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# -----------------------\n",
    "# Strategy 3: ScraperAPI\n",
    "# -----------------------\n",
    "def fetch_via_scraperapi(target_url, api_key=None):\n",
    "    \"\"\"\n",
    "    Use ScraperAPI to fetch the rendered HTML (ScraperAPI handles blocking).\n",
    "    Set SCRAPERAPI_KEY env var or pass api_key.\n",
    "    \"\"\"\n",
    "    key = api_key or SCRAPERAPI_KEY\n",
    "    if not key:\n",
    "        raise RuntimeError(\"No ScraperAPI key provided. Set SCRAPERAPI_KEY environment variable or pass api_key.\")\n",
    "\n",
    "    payload = {\n",
    "        \"api_key\": key,\n",
    "        \"url\": target_url,\n",
    "        \"country_code\": \"us\",\n",
    "        \"render\": \"false\",   # let ScraperAPI render JS\n",
    "        \"keep_headers\": \"false\"\n",
    "    }\n",
    "    url = \"http://api.scraperapi.com/?\" + urlencode(payload)\n",
    "    resp = safe_get(url, headers=pick_headers(), timeout=20)\n",
    "    if resp and resp.status_code == 200:\n",
    "        return resp.text\n",
    "    return None\n",
    "\n",
    "\n",
    "def scrape_search_scraperapi(query=\"restaurants\", location=DEFAULT_LOCATION, pages=2, api_key=None):\n",
    "    found = []\n",
    "    for p in range(pages):\n",
    "        start = p * 10\n",
    "        params = {\"find_desc\": query, \"find_loc\": location, \"start\": start}\n",
    "        target = BASE_SEARCH_URL + \"?\" + urlencode(params)\n",
    "        logger.info(\"ScraperAPI requesting %s\", target)\n",
    "        html = fetch_via_scraperapi(target, api_key=api_key)\n",
    "        if not html:\n",
    "            continue\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        for a in soup.find_all(\"a\", href=True):\n",
    "            if a[\"href\"].startswith(\"/biz/\"):\n",
    "                full = urljoin(YELP_BASE, a[\"href\"].split(\"?\")[0])\n",
    "                if full not in found:\n",
    "                    found.append(full)\n",
    "        time.sleep(1)\n",
    "    logger.info(\"ScraperAPI found %d business links\", len(found))\n",
    "    return found\n",
    "\n",
    "\n",
    "def fetch_reviews_scraperapi_old(biz_url, api_key=None):\n",
    "    \"\"\"\n",
    "    Fetch reviews for a Yelp business via ScraperAPI.\n",
    "    If latest_only=True, only include reviews newer than 'days' days.\n",
    "    \"\"\"\n",
    "    html = fetch_via_scraperapi(biz_url, api_key=api_key)\n",
    "    if not html:\n",
    "        return []\n",
    "\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    reviews = []\n",
    "    cutoff = datetime.now() - timedelta(days=days) if latest_only else None\n",
    "\n",
    "    for p in soup.find_all(\"p\"):\n",
    "        txt = p.get_text(strip=True)\n",
    "        if len(txt) > 50:\n",
    "            parent = p.parent\n",
    "            date = None\n",
    "            t = parent.find(\"time\")\n",
    "            if t and t.has_attr(\"datetime\"):\n",
    "                date = t[\"datetime\"]\n",
    "\n",
    "            # Skip older reviews if filtering\n",
    "            if latest_only and date:\n",
    "                try:\n",
    "                    parsed_date = datetime.fromisoformat(date)\n",
    "                    if parsed_date < cutoff:\n",
    "                        continue\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "            reviews.append({\n",
    "                \"biz_url\": biz_url,\n",
    "                \"review_text\": txt,\n",
    "                \"date\": date,\n",
    "                \"rating\": None\n",
    "            })\n",
    "\n",
    "    logger.info(\n",
    "        \"ScraperAPI parsed %d %sreviews for %s\",\n",
    "        len(reviews),\n",
    "        \"recent \" if latest_only else \"\",\n",
    "        biz_url,\n",
    "    )\n",
    "    return reviews\n",
    "\n",
    "def fetch_reviews_scraperapi(biz_url, api_key=None, latest_only=False, days=60):\n",
    "    \"\"\"\n",
    "    Fetch Yelp reviews for a business via ScraperAPI.\n",
    "\n",
    "    Parameters:\n",
    "    - biz_url: URL of the Yelp business page\n",
    "    - api_key: ScraperAPI key\n",
    "    - latest_only: if True, only keep reviews from the last `days`\n",
    "    - days: number of days to consider for latest_only\n",
    "    \"\"\"\n",
    "    html = fetch_via_scraperapi(biz_url, api_key=api_key)\n",
    "    if not html:\n",
    "        return []\n",
    "\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    reviews = []\n",
    "    for review_div in soup.find_all(\"div\", attrs={\"data-testid\": \"review\"}):\n",
    "        # review text\n",
    "        span = review_div.find(\"span\", {\"lang\": True})\n",
    "        text = span.get_text(\"\\n\", strip=True) if span else None\n",
    "    \n",
    "        # date\n",
    "        date_tag = review_div.find(\"span\", string=lambda s: s and (\"ago\" in s or \"on\" in s))\n",
    "        date = date_tag.get_text(strip=True) if date_tag else None\n",
    "    \n",
    "        # rating\n",
    "        rating_tag = review_div.find(\"div\", role=\"img\")\n",
    "        rating = None\n",
    "        if rating_tag and rating_tag.has_attr(\"aria-label\"):\n",
    "            rating = rating_tag[\"aria-label\"].split(\" \")[0]  # e.g. \"4 star rating\" â†’ 4\n",
    "    \n",
    "        if text:\n",
    "            reviews.append({\n",
    "                \"biz_url\": biz_url,\n",
    "                \"review_text\": text,\n",
    "                \"date\": date,\n",
    "                \"rating\": rating\n",
    "            })\n",
    "\n",
    "\n",
    "    logger.info(\"ScraperAPI parsed %d reviews for %s\", len(reviews), biz_url)\n",
    "    return reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "16f17314-3460-47d2-8a17-72c46f728517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Orchestration & helpers\n",
    "# -----------------------\n",
    "def save_reviews_csv(reviews: List[Dict], out_path=\"../data/raw/berlin_reviews_yelp.csv\"):\n",
    "    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "    keys = [\"biz_url\", \"review_text\", \"date\", \"rating\"]\n",
    "    with open(out_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=keys)\n",
    "        writer.writeheader()\n",
    "        for r in reviews:\n",
    "            writer.writerow({k: r.get(k) for k in keys})\n",
    "    logger.info(\"Saved %d reviews to %s\", len(reviews), out_path)\n",
    "\n",
    "\n",
    "def collect_restaurants(query=\"restaurants\", location=DEFAULT_LOCATION, limit_biz=20, pages=2, use_selenium=False, use_scraperapi=False):\n",
    "    \"\"\"\n",
    "    Top-level helper:\n",
    "    1) Try fast requests strategy\n",
    "    2) If 0 results and use_selenium True: try selenium\n",
    "    3) If still 0 and SCRAPERAPI key present or use_scraperapi True: try ScraperAPI\n",
    "    4) Fetch reviews for discovered business pages (first N)\n",
    "    \"\"\"\n",
    "    logger.info(\"Collecting restaurants for query=%s location=%s\", query, location)\n",
    "    businesses = scrape_search_requests(query=query, location=location, pages=pages)\n",
    "    if len(businesses) == 0 and use_selenium:\n",
    "        logger.info(\"Requests found 0 â€” trying Selenium Edge\")\n",
    "        businesses = scrape_with_selenium_edge(query=query, location=location, pages=pages)\n",
    "    if len(businesses) == 0 and (use_scraperapi or SCRAPERAPI_KEY):\n",
    "        logger.info(\"Trying ScraperAPI fallback\")\n",
    "        businesses = scrape_search_scraperapi(query=query, location=location, pages=pages, api_key=SCRAPERAPI_KEY)\n",
    "\n",
    "    businesses = businesses[:limit_biz]\n",
    "    logger.info(\"Will fetch reviews for %d businesses\", len(businesses))\n",
    "\n",
    "    all_reviews = []\n",
    "    for b in businesses:\n",
    "        # requests version (no latest_only support)\n",
    "        r = fetch_reviews_requests(b)\n",
    "        \n",
    "        # selenium fallback (if implemented with latest_only, you could pass it)\n",
    "        if not r and use_selenium:\n",
    "            r = fetch_reviews_selenium(b)\n",
    "        \n",
    "        # ScraperAPI version (supports latest_only)\n",
    "        if not r and (use_scraperapi or SCRAPERAPI_KEY):\n",
    "            r = fetch_reviews_scraperapi(\n",
    "                b,\n",
    "                api_key=SCRAPERAPI_KEY,\n",
    "                latest_only=True,  # <-- filter for latest\n",
    "                days=60            # <-- last 60 days\n",
    "            )\n",
    "        all_reviews.extend(r)\n",
    "        time.sleep(random.uniform(1.0, 2.0))\n",
    "\n",
    "\n",
    "    save_reviews_csv(all_reviews)\n",
    "    return all_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "deaa6449-f75b-4a37-8cde-fc33faf6bc32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:data_collection:Collecting restaurants for query=restaurants location=Berlin, Germany\n",
      "INFO:data_collection:Requests search page 1 (start=0)\n",
      "WARNING:data_collection:GET https://www.yelp.com/search returned status 403\n",
      "WARNING:data_collection:GET https://www.yelp.com/search returned status 403\n",
      "WARNING:data_collection:GET https://www.yelp.com/search returned status 403\n",
      "WARNING:data_collection:No response for search page 1\n",
      "INFO:data_collection:Requests search page 2 (start=10)\n",
      "WARNING:data_collection:GET https://www.yelp.com/search returned status 403\n",
      "WARNING:data_collection:GET https://www.yelp.com/search returned status 403\n",
      "WARNING:data_collection:GET https://www.yelp.com/search returned status 403\n",
      "WARNING:data_collection:No response for search page 2\n",
      "INFO:data_collection:Requests search found 0 business links\n",
      "INFO:data_collection:Trying ScraperAPI fallback\n",
      "INFO:data_collection:ScraperAPI requesting https://www.yelp.com/search?find_desc=restaurants&find_loc=Berlin%2C+Germany&start=0\n",
      "INFO:data_collection:ScraperAPI requesting https://www.yelp.com/search?find_desc=restaurants&find_loc=Berlin%2C+Germany&start=10\n",
      "INFO:data_collection:ScraperAPI found 19 business links\n",
      "INFO:data_collection:Will fetch reviews for 19 businesses\n",
      "INFO:data_collection:Parsed 5 candidate reviews from https://www.yelp.com/biz/schnitzelei-mitte-berlin\n",
      "WARNING:data_collection:GET https://www.yelp.com/biz/schl%C3%B6gls-berlin returned status 403\n",
      "WARNING:data_collection:GET https://www.yelp.com/biz/schl%C3%B6gls-berlin returned status 403\n",
      "WARNING:data_collection:GET https://www.yelp.com/biz/schl%C3%B6gls-berlin returned status 403\n",
      "INFO:data_collection:ScraperAPI parsed 0 reviews for https://www.yelp.com/biz/schl%C3%B6gls-berlin\n",
      "INFO:data_collection:Parsed 4 candidate reviews from https://www.yelp.com/biz/stadtklause-berlin\n",
      "WARNING:data_collection:GET https://www.yelp.com/biz/gasthaus-krombach-berlin returned status 403\n",
      "WARNING:data_collection:GET https://www.yelp.com/biz/gasthaus-krombach-berlin returned status 403\n",
      "WARNING:data_collection:GET https://www.yelp.com/biz/gasthaus-krombach-berlin returned status 403\n",
      "WARNING:data_collection:Request exception: HTTPConnectionPool(host='api.scraperapi.com', port=80): Read timed out. (read timeout=20)\n",
      "WARNING:data_collection:Request exception: HTTPConnectionPool(host='api.scraperapi.com', port=80): Read timed out. (read timeout=20)\n",
      "INFO:data_collection:ScraperAPI parsed 0 reviews for https://www.yelp.com/biz/gasthaus-krombach-berlin\n",
      "WARNING:data_collection:GET https://www.yelp.com/biz/gaffel-haus-berlin-das-k%C3%B6lsche-konsulat-berlin returned status 403\n",
      "INFO:data_collection:Parsed 5 candidate reviews from https://www.yelp.com/biz/gaffel-haus-berlin-das-k%C3%B6lsche-konsulat-berlin\n",
      "WARNING:data_collection:GET https://www.yelp.com/biz/hirsch-und-hase-berlin returned status 403\n",
      "WARNING:data_collection:GET https://www.yelp.com/biz/hirsch-und-hase-berlin returned status 403\n",
      "WARNING:data_collection:GET https://www.yelp.com/biz/hirsch-und-hase-berlin returned status 403\n",
      "INFO:data_collection:ScraperAPI parsed 0 reviews for https://www.yelp.com/biz/hirsch-und-hase-berlin\n",
      "INFO:data_collection:Parsed 4 candidate reviews from https://www.yelp.com/biz/hackethals-berlin\n",
      "WARNING:data_collection:GET https://www.yelp.com/biz/night-kitchen-berlin returned status 403\n",
      "INFO:data_collection:Parsed 4 candidate reviews from https://www.yelp.com/biz/night-kitchen-berlin\n",
      "WARNING:data_collection:GET https://www.yelp.com/biz/pascarella-berlin-2 returned status 403\n",
      "WARNING:data_collection:GET https://www.yelp.com/biz/pascarella-berlin-2 returned status 403\n",
      "WARNING:data_collection:GET https://www.yelp.com/biz/pascarella-berlin-2 returned status 403\n",
      "INFO:data_collection:ScraperAPI parsed 0 reviews for https://www.yelp.com/biz/pascarella-berlin-2\n",
      "WARNING:data_collection:GET https://www.yelp.com/biz/salt-n-bone-berlin-2 returned status 403\n",
      "WARNING:data_collection:GET https://www.yelp.com/biz/salt-n-bone-berlin-2 returned status 403\n",
      "WARNING:data_collection:GET https://www.yelp.com/biz/salt-n-bone-berlin-2 returned status 403\n",
      "INFO:data_collection:ScraperAPI parsed 0 reviews for https://www.yelp.com/biz/salt-n-bone-berlin-2\n",
      "WARNING:data_collection:GET https://www.yelp.com/biz/st%C3%A4ndige-vertretung-berlin-2 returned status 403\n",
      "WARNING:data_collection:GET https://www.yelp.com/biz/st%C3%A4ndige-vertretung-berlin-2 returned status 403\n",
      "INFO:data_collection:Parsed 4 candidate reviews from https://www.yelp.com/biz/st%C3%A4ndige-vertretung-berlin-2\n",
      "INFO:data_collection:Parsed 5 candidate reviews from https://www.yelp.com/biz/burgermeister-berlin\n",
      "WARNING:data_collection:GET https://www.yelp.com/biz/calice-d-oro-berlin returned status 403\n",
      "WARNING:data_collection:GET https://www.yelp.com/biz/calice-d-oro-berlin returned status 403\n",
      "WARNING:data_collection:GET https://www.yelp.com/biz/calice-d-oro-berlin returned status 403\n",
      "INFO:data_collection:ScraperAPI parsed 0 reviews for https://www.yelp.com/biz/calice-d-oro-berlin\n",
      "WARNING:data_collection:GET https://www.yelp.com/biz/acht-und-dreissig-berlin returned status 403\n",
      "WARNING:data_collection:GET https://www.yelp.com/biz/acht-und-dreissig-berlin returned status 403\n",
      "WARNING:data_collection:GET https://www.yelp.com/biz/acht-und-dreissig-berlin returned status 403\n",
      "INFO:data_collection:ScraperAPI parsed 0 reviews for https://www.yelp.com/biz/acht-und-dreissig-berlin\n",
      "WARNING:data_collection:GET https://www.yelp.com/biz/boulevard-friedrichstrasse-berlin returned status 403\n",
      "INFO:data_collection:Parsed 4 candidate reviews from https://www.yelp.com/biz/boulevard-friedrichstrasse-berlin\n",
      "INFO:data_collection:Parsed 5 candidate reviews from https://www.yelp.com/biz/takumi9-sapporo-berlin\n",
      "WARNING:data_collection:GET https://www.yelp.com/biz/elefant-berlin returned status 403\n",
      "WARNING:data_collection:GET https://www.yelp.com/biz/elefant-berlin returned status 403\n",
      "WARNING:data_collection:GET https://www.yelp.com/biz/elefant-berlin returned status 403\n",
      "INFO:data_collection:ScraperAPI parsed 0 reviews for https://www.yelp.com/biz/elefant-berlin\n",
      "INFO:data_collection:Parsed 3 candidate reviews from https://www.yelp.com/biz/sisaket-berlin-2\n",
      "WARNING:data_collection:GET https://www.yelp.com/biz/republik-berlin-berlin returned status 403\n",
      "WARNING:data_collection:GET https://www.yelp.com/biz/republik-berlin-berlin returned status 403\n",
      "WARNING:data_collection:GET https://www.yelp.com/biz/republik-berlin-berlin returned status 403\n",
      "INFO:data_collection:ScraperAPI parsed 0 reviews for https://www.yelp.com/biz/republik-berlin-berlin\n",
      "INFO:data_collection:Saved 43 reviews to ../data/raw/berlin_reviews_yelp.csv\n"
     ]
    }
   ],
   "source": [
    "reviews = collect_restaurants(\n",
    "    query=\"restaurants\",\n",
    "    location=\"Berlin, Germany\",\n",
    "    limit_biz=20,\n",
    "    pages=2,\n",
    "    use_selenium=False,   # don't attempt Selenium\n",
    "    use_scraperapi=True   # rely on ScraperAPI\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a7457a54-bb40-46a4-a84b-277649f8462a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 43 reviews\n",
      "[{'biz_url': 'https://www.yelp.com/biz/schnitzelei-mitte-berlin', 'review_text': 'â€œThey gave me awelcome beerand I ordered the Weiner Schnitzel with potatoes and the portion was huge but delicious!â€in 7 reviews', 'date': None, 'rating': None}, {'biz_url': 'https://www.yelp.com/biz/schnitzelei-mitte-berlin', 'review_text': 'â€œMy friend and I shared theGerman tapas(9 dishes for ~21 euros) and it was the perfect size lunch for us.â€in 4 reviews', 'date': None, 'rating': None}, {'biz_url': 'https://www.yelp.com/biz/schnitzelei-mitte-berlin', 'review_text': 'â€œI ordered theveal schnitzeland my friend ordered pork that came with an amazing horseradish type sauce.â€in 4 reviews', 'date': None, 'rating': None}]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Collected {len(reviews)} reviews\")\n",
    "print(reviews[:3])  # show first 3 reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "844ece42-1625-4879-98f6-1682b0bebdeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>biz_url</th>\n",
       "      <th>review_text</th>\n",
       "      <th>date</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.yelp.com/biz/schnitzelei-mitte-berlin</td>\n",
       "      <td>â€œThey gave me awelcome beerand I ordered the W...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.yelp.com/biz/schnitzelei-mitte-berlin</td>\n",
       "      <td>â€œMy friend and I shared theGerman tapas(9 dish...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.yelp.com/biz/schnitzelei-mitte-berlin</td>\n",
       "      <td>â€œI ordered theveal schnitzeland my friend orde...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.yelp.com/biz/schnitzelei-mitte-berlin</td>\n",
       "      <td>Schnitzel in allen Variationen ist unsere Spez...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.yelp.com/biz/schnitzelei-mitte-berlin</td>\n",
       "      <td>Copyright Â© 2004â€“2025 Yelp Inc. Yelp, Elite Sq...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>https://www.yelp.com/biz/stadtklause-berlin</td>\n",
       "      <td>â€œThe menu changes often, but theStrammer Maxis...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>https://www.yelp.com/biz/stadtklause-berlin</td>\n",
       "      <td>â€œLimited menu, but the menu is available inEng...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>https://www.yelp.com/biz/stadtklause-berlin</td>\n",
       "      <td>â€œFood: I ordered a Pilsner with thepork schnit...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>https://www.yelp.com/biz/stadtklause-berlin</td>\n",
       "      <td>Copyright Â© 2004â€“2025 Yelp Inc. Yelp, Elite Sq...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>https://www.yelp.com/biz/gaffel-haus-berlin-da...</td>\n",
       "      <td>â€œDuring our three days stay in Berlin during t...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>https://www.yelp.com/biz/gaffel-haus-berlin-da...</td>\n",
       "      <td>â€œThe menu is printed inEnglishand has a huge s...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>https://www.yelp.com/biz/gaffel-haus-berlin-da...</td>\n",
       "      <td>â€œSurprise surprise, but theirhouse beerwas als...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>https://www.yelp.com/biz/gaffel-haus-berlin-da...</td>\n",
       "      <td>Rheinische Gastfreundschaft im Herzen Berlins ...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>https://www.yelp.com/biz/gaffel-haus-berlin-da...</td>\n",
       "      <td>Copyright Â© 2004â€“2025 Yelp Inc. Yelp, Elite Sq...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>https://www.yelp.com/biz/hackethals-berlin</td>\n",
       "      <td>â€œI had the german specials of beef in a wine r...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>https://www.yelp.com/biz/hackethals-berlin</td>\n",
       "      <td>â€œThe best slow roastedvenisoni have had in my ...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>https://www.yelp.com/biz/hackethals-berlin</td>\n",
       "      <td>â€œThey apparently get enough tourists that they...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>https://www.yelp.com/biz/hackethals-berlin</td>\n",
       "      <td>Copyright Â© 2004â€“2025 Yelp Inc. Yelp, Elite Sq...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>https://www.yelp.com/biz/night-kitchen-berlin</td>\n",
       "      <td>â€œInstead of doing the Dinner with Friends opti...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>https://www.yelp.com/biz/night-kitchen-berlin</td>\n",
       "      <td>â€œHighlyrecommend the steak tartare, the artich...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>https://www.yelp.com/biz/night-kitchen-berlin</td>\n",
       "      <td>â€œEnjoyed theChallah bread, broccolini, crispy ...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>https://www.yelp.com/biz/night-kitchen-berlin</td>\n",
       "      <td>Copyright Â© 2004â€“2025 Yelp Inc. Yelp, Elite Sq...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>https://www.yelp.com/biz/st%C3%A4ndige-vertret...</td>\n",
       "      <td>â€œI lovematjesand theirs are excellent (good sa...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>https://www.yelp.com/biz/st%C3%A4ndige-vertret...</td>\n",
       "      <td>â€œI opted for theCurrywurstwhich turned out to ...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>https://www.yelp.com/biz/st%C3%A4ndige-vertret...</td>\n",
       "      <td>â€œA beef in gravy with red cabbage and big pota...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>https://www.yelp.com/biz/st%C3%A4ndige-vertret...</td>\n",
       "      <td>Copyright Â© 2004â€“2025 Yelp Inc. Yelp, Elite Sq...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>https://www.yelp.com/biz/burgermeister-berlin</td>\n",
       "      <td>â€œI ordered themeisterburger, which had bacon, ...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>https://www.yelp.com/biz/burgermeister-berlin</td>\n",
       "      <td>â€œI get theHausmeister Burgerand I share my che...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>https://www.yelp.com/biz/burgermeister-berlin</td>\n",
       "      <td>â€œI never expected one of the best burgers I'd ...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>https://www.yelp.com/biz/burgermeister-berlin</td>\n",
       "      <td>Frisch zubereitete Burger in einem ehemaligen ...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>https://www.yelp.com/biz/burgermeister-berlin</td>\n",
       "      <td>Copyright Â© 2004â€“2025 Yelp Inc. Yelp, Elite Sq...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>https://www.yelp.com/biz/boulevard-friedrichst...</td>\n",
       "      <td>â€œWe ordered the potato soup,mixed grilland the...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>https://www.yelp.com/biz/boulevard-friedrichst...</td>\n",
       "      <td>â€œThe food was spectacular, and our waiter (Pet...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>https://www.yelp.com/biz/boulevard-friedrichst...</td>\n",
       "      <td>â€œCharming place near theriverand Friedrichstra...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>https://www.yelp.com/biz/boulevard-friedrichst...</td>\n",
       "      <td>Copyright Â© 2004â€“2025 Yelp Inc. Yelp, Elite Sq...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>https://www.yelp.com/biz/takumi9-sapporo-berlin</td>\n",
       "      <td>â€œAnyone that knowsTakumiin DÃ¼sseldorf is proba...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>https://www.yelp.com/biz/takumi9-sapporo-berlin</td>\n",
       "      <td>â€œHighlyrec the tan tan ramen\\nThere might be a...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>https://www.yelp.com/biz/takumi9-sapporo-berlin</td>\n",
       "      <td>â€œI also ordered gyoza, karaage chicken andedam...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>https://www.yelp.com/biz/takumi9-sapporo-berlin</td>\n",
       "      <td>People searched for Ramen 133 times last month...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>https://www.yelp.com/biz/takumi9-sapporo-berlin</td>\n",
       "      <td>Copyright Â© 2004â€“2025 Yelp Inc. Yelp, Elite Sq...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>https://www.yelp.com/biz/sisaket-berlin-2</td>\n",
       "      <td>â€œWenthere solo for lunch as I couldn't eat any...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>https://www.yelp.com/biz/sisaket-berlin-2</td>\n",
       "      <td>People searched for Thai 117 times last month ...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>https://www.yelp.com/biz/sisaket-berlin-2</td>\n",
       "      <td>Copyright Â© 2004â€“2025 Yelp Inc. Yelp, Elite Sq...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              biz_url  \\\n",
       "0   https://www.yelp.com/biz/schnitzelei-mitte-berlin   \n",
       "1   https://www.yelp.com/biz/schnitzelei-mitte-berlin   \n",
       "2   https://www.yelp.com/biz/schnitzelei-mitte-berlin   \n",
       "3   https://www.yelp.com/biz/schnitzelei-mitte-berlin   \n",
       "4   https://www.yelp.com/biz/schnitzelei-mitte-berlin   \n",
       "5         https://www.yelp.com/biz/stadtklause-berlin   \n",
       "6         https://www.yelp.com/biz/stadtklause-berlin   \n",
       "7         https://www.yelp.com/biz/stadtklause-berlin   \n",
       "8         https://www.yelp.com/biz/stadtklause-berlin   \n",
       "9   https://www.yelp.com/biz/gaffel-haus-berlin-da...   \n",
       "10  https://www.yelp.com/biz/gaffel-haus-berlin-da...   \n",
       "11  https://www.yelp.com/biz/gaffel-haus-berlin-da...   \n",
       "12  https://www.yelp.com/biz/gaffel-haus-berlin-da...   \n",
       "13  https://www.yelp.com/biz/gaffel-haus-berlin-da...   \n",
       "14         https://www.yelp.com/biz/hackethals-berlin   \n",
       "15         https://www.yelp.com/biz/hackethals-berlin   \n",
       "16         https://www.yelp.com/biz/hackethals-berlin   \n",
       "17         https://www.yelp.com/biz/hackethals-berlin   \n",
       "18      https://www.yelp.com/biz/night-kitchen-berlin   \n",
       "19      https://www.yelp.com/biz/night-kitchen-berlin   \n",
       "20      https://www.yelp.com/biz/night-kitchen-berlin   \n",
       "21      https://www.yelp.com/biz/night-kitchen-berlin   \n",
       "22  https://www.yelp.com/biz/st%C3%A4ndige-vertret...   \n",
       "23  https://www.yelp.com/biz/st%C3%A4ndige-vertret...   \n",
       "24  https://www.yelp.com/biz/st%C3%A4ndige-vertret...   \n",
       "25  https://www.yelp.com/biz/st%C3%A4ndige-vertret...   \n",
       "26      https://www.yelp.com/biz/burgermeister-berlin   \n",
       "27      https://www.yelp.com/biz/burgermeister-berlin   \n",
       "28      https://www.yelp.com/biz/burgermeister-berlin   \n",
       "29      https://www.yelp.com/biz/burgermeister-berlin   \n",
       "30      https://www.yelp.com/biz/burgermeister-berlin   \n",
       "31  https://www.yelp.com/biz/boulevard-friedrichst...   \n",
       "32  https://www.yelp.com/biz/boulevard-friedrichst...   \n",
       "33  https://www.yelp.com/biz/boulevard-friedrichst...   \n",
       "34  https://www.yelp.com/biz/boulevard-friedrichst...   \n",
       "35    https://www.yelp.com/biz/takumi9-sapporo-berlin   \n",
       "36    https://www.yelp.com/biz/takumi9-sapporo-berlin   \n",
       "37    https://www.yelp.com/biz/takumi9-sapporo-berlin   \n",
       "38    https://www.yelp.com/biz/takumi9-sapporo-berlin   \n",
       "39    https://www.yelp.com/biz/takumi9-sapporo-berlin   \n",
       "40          https://www.yelp.com/biz/sisaket-berlin-2   \n",
       "41          https://www.yelp.com/biz/sisaket-berlin-2   \n",
       "42          https://www.yelp.com/biz/sisaket-berlin-2   \n",
       "\n",
       "                                          review_text  date rating  \n",
       "0   â€œThey gave me awelcome beerand I ordered the W...  None   None  \n",
       "1   â€œMy friend and I shared theGerman tapas(9 dish...  None   None  \n",
       "2   â€œI ordered theveal schnitzeland my friend orde...  None   None  \n",
       "3   Schnitzel in allen Variationen ist unsere Spez...  None   None  \n",
       "4   Copyright Â© 2004â€“2025 Yelp Inc. Yelp, Elite Sq...  None   None  \n",
       "5   â€œThe menu changes often, but theStrammer Maxis...  None   None  \n",
       "6   â€œLimited menu, but the menu is available inEng...  None   None  \n",
       "7   â€œFood: I ordered a Pilsner with thepork schnit...  None   None  \n",
       "8   Copyright Â© 2004â€“2025 Yelp Inc. Yelp, Elite Sq...  None   None  \n",
       "9   â€œDuring our three days stay in Berlin during t...  None   None  \n",
       "10  â€œThe menu is printed inEnglishand has a huge s...  None   None  \n",
       "11  â€œSurprise surprise, but theirhouse beerwas als...  None   None  \n",
       "12  Rheinische Gastfreundschaft im Herzen Berlins ...  None   None  \n",
       "13  Copyright Â© 2004â€“2025 Yelp Inc. Yelp, Elite Sq...  None   None  \n",
       "14  â€œI had the german specials of beef in a wine r...  None   None  \n",
       "15  â€œThe best slow roastedvenisoni have had in my ...  None   None  \n",
       "16  â€œThey apparently get enough tourists that they...  None   None  \n",
       "17  Copyright Â© 2004â€“2025 Yelp Inc. Yelp, Elite Sq...  None   None  \n",
       "18  â€œInstead of doing the Dinner with Friends opti...  None   None  \n",
       "19  â€œHighlyrecommend the steak tartare, the artich...  None   None  \n",
       "20  â€œEnjoyed theChallah bread, broccolini, crispy ...  None   None  \n",
       "21  Copyright Â© 2004â€“2025 Yelp Inc. Yelp, Elite Sq...  None   None  \n",
       "22  â€œI lovematjesand theirs are excellent (good sa...  None   None  \n",
       "23  â€œI opted for theCurrywurstwhich turned out to ...  None   None  \n",
       "24  â€œA beef in gravy with red cabbage and big pota...  None   None  \n",
       "25  Copyright Â© 2004â€“2025 Yelp Inc. Yelp, Elite Sq...  None   None  \n",
       "26  â€œI ordered themeisterburger, which had bacon, ...  None   None  \n",
       "27  â€œI get theHausmeister Burgerand I share my che...  None   None  \n",
       "28  â€œI never expected one of the best burgers I'd ...  None   None  \n",
       "29  Frisch zubereitete Burger in einem ehemaligen ...  None   None  \n",
       "30  Copyright Â© 2004â€“2025 Yelp Inc. Yelp, Elite Sq...  None   None  \n",
       "31  â€œWe ordered the potato soup,mixed grilland the...  None   None  \n",
       "32  â€œThe food was spectacular, and our waiter (Pet...  None   None  \n",
       "33  â€œCharming place near theriverand Friedrichstra...  None   None  \n",
       "34  Copyright Â© 2004â€“2025 Yelp Inc. Yelp, Elite Sq...  None   None  \n",
       "35  â€œAnyone that knowsTakumiin DÃ¼sseldorf is proba...  None   None  \n",
       "36  â€œHighlyrec the tan tan ramen\\nThere might be a...  None   None  \n",
       "37  â€œI also ordered gyoza, karaage chicken andedam...  None   None  \n",
       "38  People searched for Ramen 133 times last month...  None   None  \n",
       "39  Copyright Â© 2004â€“2025 Yelp Inc. Yelp, Elite Sq...  None   None  \n",
       "40  â€œWenthere solo for lunch as I couldn't eat any...  None   None  \n",
       "41  People searched for Thai 117 times last month ...  None   None  \n",
       "42  Copyright Â© 2004â€“2025 Yelp Inc. Yelp, Elite Sq...  None   None  "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "35577bc0-a403-487b-b346-f58d0863ac8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewiewsDF = pd.DataFrame(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9273e3d3-faca-4286-ab4f-8c00ffcb536e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'â€œMy friend and I shared theGerman tapas(9 dishes for ~21 euros) and it was the perfect size lunch for us.â€in 4 reviews'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewiewsDF.review_text[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
